{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b11PMt8r3rHd"
      },
      "source": [
        "![](https://img.shields.io/badge/PO.DAAC-Contribution-%20?color=grey&labelColor=blue)\n",
        "\n",
        "> From the PO.DAAC Cookbook, to access the GitHub version of the notebook, follow [this link](https://github.com/podaac/tutorials/blob/master/notebooks/datasets/SWOTHR_localmachine.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNpTEGt3rHe"
      },
      "source": [
        "# SWOT Hydrology Dataset Exploration on a local machine\n",
        "\n",
        "## Accessing and Visualizing SWOT Datasets\n",
        "\n",
        "### Requirement:\n",
        "Local compute environment e.g. laptop, server: this tutorial can be run on your local machine.\n",
        "\n",
        "### Learning Objectives:\n",
        "- Access SWOT HR data prodcuts (archived in NASA Earthdata Cloud) within the AWS cloud, by downloading to local machine\n",
        "- Visualize accessed data for a quick check\n",
        "\n",
        "#### SWOT Level 2 KaRIn High Rate Version 2.0 Datasets:\n",
        "\n",
        "1. **River Vector Shapefile** - SWOT_L2_HR_RIVERSP_2.0\n",
        "\n",
        "2. **Lake Vector Shapefile** - SWOT_L2_HR_LAKESP_2.0\n",
        "\n",
        "3. **Water Mask Pixel Cloud NetCDF** - SWOT_L2_HR_PIXC_2.0\n",
        "\n",
        "4. **Water Mask Pixel Cloud Vector Attribute NetCDF** - SWOT_L2_HR_PIXCVec_2.0\n",
        "\n",
        "5. **Raster NetCDF** - SWOT_L2_HR_Raster_2.0\n",
        "\n",
        "6. **Single Look Complex Data product** - SWOT_L1B_HR_SLC_2.0\n",
        "\n",
        "_This notebook has been slightly modified by the University of Sherbrooke and University Laval team to run smoothly in Google Colab for the June 10, 2024 training session. Original authors :  Cassie Nickles, NASA PO.DAAC (Feb 2024) || Other Contributors: Zoe Walschots (PO.DAAC Summer Intern 2023), Catalina Taglialatela (NASA PO.DAAC), Luis Lopez (NASA NSIDC DAAC)_\n",
        "\n",
        "_Last update :  January 13, 2026_\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60WhXudr3rHe"
      },
      "source": [
        "### Libraries Needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iUtXlwn3rHe"
      },
      "outputs": [],
      "source": [
        "!pip install contextily\n",
        "!pip install earthaccess\n",
        "!pip install --upgrade holoviews hvplot\n",
        "!pip install holoviews hvplot bokeh xarray\n",
        "!pip install rioxarray\n",
        "!pip install rasterio\n",
        "!pip install shapely\n",
        "!pip install geoviews\n",
        "!pip install pyproj\n",
        "#!pip install csrspy\n",
        "\n",
        "#!pip install hvplot\n",
        "\n",
        "\n",
        "import glob\n",
        "import h5netcdf\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import contextily as cx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import hvplot.xarray\n",
        "import holoviews as hv\n",
        "import zipfile\n",
        "import earthaccess\n",
        "import os\n",
        "import rioxarray\n",
        "from shapely.geometry import mapping\n",
        "from shapely.geometry import Point\n",
        "import csv\n",
        "import shapefile\n",
        "import geoviews as gvts\n",
        "from pyproj import Proj\n",
        "import logging\n",
        "from shapely.geometry import box\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from csrspy.main import CSRSTransformer\n",
        "#from csrspy.enums import CoordType, Reference, VerticalDatum\n",
        "#from csrspy.utils import sync_missing_grid_files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Itln68qJf11K"
      },
      "source": [
        "Install modified csrspy library for transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdt3MdKMf2kR"
      },
      "outputs": [],
      "source": [
        "# Download and extract the library\n",
        "!wget -O csrspy_modifie.zip https://github.com/sfoucher/SWOT-Canada/raw/main/csrspy_modifie.zip\n",
        "!unzip -o csrspy_modifie.zip -d csrspy_modifie\n",
        "\n",
        "# Add the folder to the Python path\n",
        "import sys\n",
        "sys.path.append('/content/csrspy_modifie')\n",
        "\n",
        "# Verify contents\n",
        "!ls /content/csrspy_modifie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVuCN_v2f42h"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import the package\n",
        "import sys\n",
        "sys.path.append('/content/csrspy_modifie/csrspy_modifie')\n",
        "import csrspy\n",
        "from csrspy.main import CSRSTransformer\n",
        "from csrspy.enums import CoordType, Reference, VerticalDatum\n",
        "from csrspy.utils import sync_missing_grid_files\n",
        "dir(csrspy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUjA7H1Y3rHf"
      },
      "source": [
        "### Earthdata Login\n",
        "\n",
        "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. If you don't already have one, please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up. We use `earthaccess` to authenticate your login credentials below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqbEXA7x3rHf"
      },
      "outputs": [],
      "source": [
        "auth = earthaccess.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4soPmS-Jbd"
      },
      "source": [
        "### Single File Access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEcoud9p3rHf"
      },
      "source": [
        "#### **1. River Vector Shapefiles**\n",
        "\n",
        "The https access link can be found using `earthaccess` data search. Since this collection consists of Reach and Node files, we need to extract only the granule for the Reach file. We do this by filtering for the 'Reach' title in the data link.\n",
        "\n",
        "Alternatively, Earthdata Search [(see tutorial)](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/tutorials/01_Earthdata_Search.html) can be used to manually search in a GUI interface.\n",
        "\n",
        "For additional tips on spatial searching of SWOT HR L2 data, see also [PO.DAAC Cookbook - SWOT Chapter tips section](https://podaac.github.io/tutorials/quarto_text/SWOT.html#tips-for-swot-hr-spatial-search).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RoIlpJpigZ-"
      },
      "source": [
        "#### Search for the data of interest\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwp1-ZHV3rHg"
      },
      "outputs": [],
      "source": [
        "#Retrieving granules with the desired characteristics using the 'earthdata.search'_data function\n",
        "river_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_RiverSP_D', # Enter 'SWOT_L2_HR_RiverSP_2.0' for the version C data\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2025-11-03 23:59:59'), # can also be filtered based on the time range\n",
        "                                        granule_name = '*Node*_214_NA*') # Here we filter by Node files (not Reach), by pass, and by continent\n",
        "                                                                        # Specify 'Node' or 'Reach' depending on the desired file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP4AgA4x0KAK"
      },
      "outputs": [],
      "source": [
        "# Print the properties of the granules associated with the selected pass\n",
        "print(river_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGGZ2Jxu3rHg"
      },
      "source": [
        "#### Dowload, unzip, read the data\n",
        "\n",
        "Let's download the selected data file! `earthaccess.download` has a list as the input format, so we need to put brackets around the single file we pass.\n",
        "Here, we download the most recent file from the collection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz2o8DL83rHg"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([river_results[-1]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr6Tvrdb3rHg"
      },
      "source": [
        "The native format for this data is a .zip file, and we want the .shp file within the .zip file, so we must first extract the data to open it. First, we'll programmatically get the filename we just downloaded, and then extract all data to the `data_downloads` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed_mTohH3rHg"
      },
      "outputs": [],
      "source": [
        "filename = earthaccess.results.DataGranule.data_links(river_results[-1], access='external')\n",
        "filename = filename[0].split(\"/\")[-1]\n",
        "filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW2hQ-CF3rHg"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(f'data_downloads/{filename}', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_downloads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE5Ge14q3rHh"
      },
      "source": [
        "Open the shapefile using `geopandas`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6RwizoQ3rHh"
      },
      "outputs": [],
      "source": [
        "filename_shp = filename.replace('.zip','.shp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5dZOp-O3rHh"
      },
      "outputs": [],
      "source": [
        "SWOT_HR_shp1 = gpd.read_file(f'data_downloads/{filename_shp}')\n",
        "\n",
        "#view the attribute table\n",
        "SWOT_HR_shp1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJHd9KST3rHh"
      },
      "source": [
        "#### Quickly plot the SWOT river data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr7C9k3PSf__"
      },
      "source": [
        "Display water elevations (WSE, Water Surface Elevation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TZRnkq7Rf6D"
      },
      "outputs": [],
      "source": [
        "# Définir les coordonnées de la boîte englobante (xmin, ymin, xmax, ymax)\n",
        "bounding_box = box(-67.01, 45.82, -66.42, 46.4)\n",
        "\n",
        "# Filtrer les valeurs aberrantes (-999999999999) dans la colonne 'wse'\n",
        "SWOT_HR_shp1_filtered = SWOT_HR_shp1[SWOT_HR_shp1['wse'] != -999999999999]\n",
        "\n",
        "# Découper le shapefile avec la boîte englobante\n",
        "SWOT_HR_shp1_clipped = gpd.clip(SWOT_HR_shp1_filtered, bounding_box)\n",
        "\n",
        "# Affichage avec WSE après découpage et filtrage des valeurs aberrantes\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# Tracer la figure avec des couleurs basées sur la colonne WSE\n",
        "SWOT_HR_shp1_clipped.plot(ax=ax, column='wse', cmap='cool', legend=True,\n",
        "                          legend_kwds={'label': \"Water Surface Elevation (WSE)\", 'orientation': \"vertical\"})\n",
        "\n",
        "# Ajouter une carte de base\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp1_clipped.crs, source=cx.providers.Esri.WorldStreetMap)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6VyAS5CSst4"
      },
      "source": [
        "Display node quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muk55soidEct"
      },
      "outputs": [],
      "source": [
        "import matplotlib.colors as mcolors\n",
        "# Create a list of unique values in node_q\n",
        "unique_values = np.sort(SWOT_HR_shp1['node_q'].unique())\n",
        "\n",
        "# Create a discret colormap\n",
        "cmap = plt.get_cmap('RdYlGn_r', len(unique_values))\n",
        "norm = mcolors.BoundaryNorm(boundaries=np.arange(len(unique_values)+1)-0.5, ncolors=len(unique_values))\n",
        "\n",
        "# Display with coloring based on node_q\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# Plot figure colored by node_q\n",
        "SWOT_HR_shp1.plot(ax=ax, column='node_q', cmap=cmap, norm=norm, legend=False)\n",
        "\n",
        "# Add legend\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "\n",
        "cbar = plt.colorbar(sm, ax=ax, ticks=range(len(unique_values)))\n",
        "cbar.ax.set_yticklabels(unique_values)\n",
        "cbar.set_label('Nodes quality')\n",
        "\n",
        "# Define area of interest boundaries\n",
        "ax.set_ylim(45.82, 46.4)\n",
        "ax.set_xlim(-67.01, -66.42)\n",
        "\n",
        "\n",
        "# Add basemap\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp1.crs, source=cx.providers.Esri.WorldStreetMap)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"0 = good\n",
        "1 = suspect - may have large errors\n",
        "2 = degraded - very likely do have large errors\n",
        "3 = bad -  may be nonsensicial and should be ignored\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DndxI6sA3rHh"
      },
      "outputs": [],
      "source": [
        "# Another way to plot geopandas dataframes is with `explore`, which also plots a basemap\n",
        "#SWOT_HR_shp1.explore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "803Ughb13rHh"
      },
      "source": [
        "#### **2. Lake Vector Shapefiles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhrHLXfL3rHh"
      },
      "source": [
        "The lake vector shapefiles can be accessed in the same way as the river shapefiles above.\n",
        "\n",
        "For additional tips on spatial searching of SWOT HR L2 data, see also [PO.DAAC Cookbook - SWOT Chapter tips section](https://podaac.github.io/tutorials/quarto_text/SWOT.html#tips-for-swot-hr-spatial-search)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTdxr0-43rHh"
      },
      "source": [
        "#### Search for data of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlHMg6UO3rHh"
      },
      "outputs": [],
      "source": [
        "lake_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_LAKESP_D',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # can also be filtered based on the time range\n",
        "                                        granule_name = '*Prior*_214_NA*') # Here we filter files with 'Prior' (this collection has three options: Obs, Unassigned, and Prior), by pass, and by continent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJYhv_hb__7M"
      },
      "outputs": [],
      "source": [
        "#Print the granule characteristics associated with the selected pass.\n",
        "print(lake_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uay3D0DS3rHi"
      },
      "source": [
        "Let's download the selected data file! For the formation purposes, let's download a granule from the Halifax region. earthaccess.download has a list as the input format, so we need to put brackets around the single file we pass.  Here, we download the most recent file from the collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WvT4rEh3rHi"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([lake_results[-1]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "barmJ-eQ3rHi"
      },
      "source": [
        "The native format for this data is a .zip file, and we want the .shp file within the .zip file, so we must first extract the data to open it. First, we'll programmatically get the filename we just downloaded, and then extract all data to the `SWOT_downloads` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uHEgBWL3rHi"
      },
      "outputs": [],
      "source": [
        "filename2 = earthaccess.results.DataGranule.data_links(lake_results[-1], access='external')\n",
        "filename2 = filename2[0].split(\"/\")[-1]\n",
        "filename2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL9U2jls3rHi"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(f'data_downloads/{filename2}', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_downloads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx4N4fCr3rHi"
      },
      "source": [
        "Open the shapefile using `geopandas`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB0mMazr3rHi"
      },
      "outputs": [],
      "source": [
        "filename_shp2 = filename2.replace('.zip','.shp')\n",
        "filename_shp2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T03QYmUd3rHi"
      },
      "outputs": [],
      "source": [
        "SWOT_HR_shp2 = gpd.read_file(f'data_downloads/{filename_shp2}')\n",
        "\n",
        "#view attribute table\n",
        "SWOT_HR_shp2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbPu8_GA3rHi"
      },
      "source": [
        "#### Quickly plot the SWOT lakes data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0a6e0sIbZnt"
      },
      "source": [
        "Display water elevations (WSE, Water Surface Elevation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Znw8nzQtdMvI"
      },
      "outputs": [],
      "source": [
        "# Filter out outliers and set the bounding box\n",
        "SWOT_HR_shp2_clipped = gpd.clip(SWOT_HR_shp2.query('wse != -999999999999'), box(-67.10, 45.47, -66.82, 45.65)) # oromocto lake\n",
        "\n",
        "# Display the clipped shapefile\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "\n",
        "\n",
        "# Plot the figure with colors based on the WSE column\n",
        "SWOT_HR_shp2_clipped.plot(ax=ax, column='wse', cmap='cool', legend=True,\n",
        "                          legend_kwds={'label': \"Niveau de d'élévation de l'eau (WSE)\", 'orientation': \"vertical\"})\n",
        "\n",
        "\n",
        "# Add WSE values to the shapefile\n",
        "centroids = SWOT_HR_shp2_clipped.geometry.centroid\n",
        "for x, y, label in zip(centroids.x, centroids.y, SWOT_HR_shp2_clipped['wse']):\n",
        "    ax.text(x, y, f'{label:.2f}', fontsize=8, ha='center', va='center', color='black')\n",
        "\n",
        "# Add basemap\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp2_clipped.crs, source=cx.providers.Esri.WorldStreetMap)\n",
        "\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvI6sAs9bhnM"
      },
      "source": [
        "Display the percentage of dark water.\n",
        "Variable dark_frac: fraction of the total lake area (area_total) covered by dark water. This value ranges from 0 to 1, where 0 indicates no dark water and 1 indicates 100% dark water.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mogzikk6fsHC"
      },
      "outputs": [],
      "source": [
        "# Filter out outliers and set the bounding box\n",
        "SWOT_HR_shp2_clipped = gpd.clip(SWOT_HR_shp2.query('wse != -999999999999'),  box(-67.10, 45.47, -66.82, 45.65))\n",
        "\n",
        "# Display the clipped shapefile\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "\n",
        "\n",
        "# Plot the figure with colors based on the dark_frac column\n",
        "SWOT_HR_shp2_clipped.plot(ax=ax, column='dark_frac', cmap='RdYlGn_r', legend=True,\n",
        "                          legend_kwds={'label': \"Pourcentage de dark water\", 'orientation': \"vertical\"})\n",
        "\n",
        "# Add dark_frac values to the shapefile\n",
        "\n",
        "centroids = SWOT_HR_shp2_clipped.geometry.centroid\n",
        "for x, y, label in zip(centroids.x, centroids.y, SWOT_HR_shp2_clipped['dark_frac']):\n",
        "    ax.text(x, y, f'{label:.2f}', fontsize=8, ha='center', va='center', color='black')\n",
        "\n",
        "# Add basemap\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp2_clipped.crs, source=cx.providers.Esri.WorldStreetMap)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8FCTuFj3rHi"
      },
      "source": [
        "Accessing the remaining files is different than the shp files above. We do not need to extract the shapefiles from a zip file because the following SWOT HR collections are stored in **netCDF** files in the cloud. For the rest of the products, we will open via `xarray`, not `geopandas`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5VVWM3T3rHj"
      },
      "source": [
        "#### **3. Water Mask Pixel Cloud NetCDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyzubIWy3rHj"
      },
      "source": [
        "#### Search for data collection and time of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI7j-xMH3rHj"
      },
      "outputs": [],
      "source": [
        "pixc_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_PIXC_D', # Enter 'SWOT_L2_HR_PIXC_2.0' for Version C (data before May 6, 2025)\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # can also specify by time\n",
        "                                        granule_name = '*_214_074L*') # pass number, tile number and swath side (R or L)\n",
        "                                        #bounding_box = (-72.73,46.58,-72.60,46.62)) # filter by bounding box, to find your bounding box : http://bboxfinder.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo-3hTY45bdY"
      },
      "outputs": [],
      "source": [
        "#Print the granule characteristics associated with the selected pass, tile and swath.\n",
        "print(pixc_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT6c7eod3rHj"
      },
      "source": [
        "Let's download one data file! earthaccess.download has a list as the input format, so we need to put brackets around the single file we pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou7k7iha3rHj"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([pixc_results[-1]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQE3zVRR3rHm"
      },
      "source": [
        "#### Open data using xarray\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gPsWBV_3rHm"
      },
      "source": [
        "The pixel cloud netCDF files are formatted with three groups titled, \"pixel cloud\", \"tvp\", or \"noise\" (more detail [here](https://podaac-tools.jpl.nasa.gov/drive/files/misc/web/misc/swot_mission_docs/pdd/D-56411_SWOT_Product_Description_L2_HR_PIXC_20200810.pdf)). In order to access the coordinates and variables within the file, a group must be specified when calling xarray open_dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKmO1lj63rHm"
      },
      "outputs": [],
      "source": [
        "ds_PIXC = xr.open_mfdataset(\"data_downloads/SWOT_L2_HR_PIXC_*.nc\", group = 'pixel_cloud', engine='h5netcdf') #If several PIXC files are uploaded to the data_downlaod file, specify which of the files to display\n",
        "ds_PIXC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM7lzUnr3rHm"
      },
      "source": [
        "#### Simple plot of the results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This could take a few minutes to plot (approx 5 m)\n",
        "vmin, vmax = np.nanpercentile(ds_PIXC.height, [2, 98])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "cax = ax.scatter(\n",
        "    ds_PIXC.longitude,\n",
        "    ds_PIXC.latitude,\n",
        "    c=ds_PIXC.height,\n",
        "    s=1,\n",
        "    vmin=vmin,\n",
        "    vmax=vmax,\n",
        "    cmap='viridis'\n",
        ")\n",
        "\n",
        "cbar = fig.colorbar(cax, ax=ax, shrink=0.5)\n",
        "cbar.set_label('Height (m)')\n",
        "\n",
        "cx.add_basemap(ax, crs='EPSG:4326', source=cx.providers.Esri.WorldStreetMap)"
      ],
      "metadata": {
        "id": "LMuACo8iHEvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-NrXv3p859"
      },
      "source": [
        "#### Clip and convert PIXC to .shp for QGIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JVeL550CcN2"
      },
      "outputs": [],
      "source": [
        "# Delete Fiona loggs\n",
        "logging.getLogger('fiona').setLevel(logging.ERROR)\n",
        "\n",
        "# Bounding box coordinates\n",
        "lat_min = 45.88\n",
        "lat_max = 45.98\n",
        "lon_min = -66.77\n",
        "lon_max = -66.49\n",
        "\n",
        "# Extract latitude and longitude\n",
        "lat = np.asarray(ds_PIXC.latitude[:])\n",
        "lon = np.asarray(ds_PIXC.longitude[:])\n",
        "classif  = np.asarray(ds_PIXC.classification[:])\n",
        "\n",
        "# Define the mask based on coordinates and classification\n",
        "mask = (lat > lat_min) & (lat < lat_max) & (lon > lon_min) & (lon < lon_max) & (classif>2) & (classif<5)\n",
        "\n",
        "# Create a dictionary with the desired variables\n",
        "data = {\n",
        "    'height': np.asarray(ds_PIXC.height[:])[mask],\n",
        "    'classif': np.asarray(ds_PIXC.classification[:])[mask],\n",
        "    'latitude': lat[mask],\n",
        "    'longitude': lon[mask]\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a dataframe\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create geometries & GeoDataFrames\n",
        "points = [Point(x, y) for x, y in zip(df.longitude, df.latitude)]\n",
        "gdf_out = gpd.GeoDataFrame(df, geometry=points, crs=\"EPSG:4326\")\n",
        "\n",
        "# Save as shapefile\n",
        "out_shp = './data_downloads/PIXC_clipped.shp'\n",
        "gdf_out.to_file(out_shp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display your clipped PIXC data. You can also download them to visualize in QGIS.\n",
        "vmin, vmax = np.nanpercentile(gdf_out.height, [2, 98])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "cax = ax.scatter(\n",
        "    gdf_out.longitude,\n",
        "    gdf_out.latitude,\n",
        "    c=gdf_out.height,\n",
        "    s=1,\n",
        "    vmin=vmin,\n",
        "    vmax=vmax,\n",
        "    cmap='viridis',\n",
        "    rasterized=True\n",
        ")\n",
        "\n",
        "cbar = fig.colorbar(cax, ax=ax, shrink=0.5)\n",
        "cbar.set_label('Height (m)')\n",
        "\n",
        "cx.add_basemap(ax, crs='EPSG:4326', source=cx.providers.Esri.WorldStreetMap)"
      ],
      "metadata": {
        "id": "OegolomkIlSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulakBaLH3rHm"
      },
      "source": [
        "#### **4. Water Mask Pixel Cloud Vector Attribute NetCDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkTelbKM3rHm"
      },
      "source": [
        "#### Search for data of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqAU4RuT3rHn"
      },
      "outputs": [],
      "source": [
        "pixcvec_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_PIXCVEC_D', # enter 'SWOT_L2_HR_PIXCVEC_2.0' for version C (data before May 6, 2026)\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # # can also specify by time\n",
        "                                        granule_name = '*_214_074L*') # pass number, tile number and swath side (R or L)\n",
        "                                        #bounding_box = (-72.73,46.58,-72.60,46.62)) # filter by bounding box, to find your bounding box : http://bboxfinder.com/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26uTWPEj7CMv"
      },
      "outputs": [],
      "source": [
        "#Print the granule characteristics.\n",
        "print(pixcvec_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkwvQI1i3rHn"
      },
      "source": [
        "Let's download the first data file! earthaccess.download has a list as the input format, so we need to put brackets around the single file we pass. Here, we download the most recent file from the collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SipHO_Lh3rHn"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([pixcvec_results[-1]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj1MBbdx3rHn"
      },
      "source": [
        "#### Open data using xarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkufSBay3rHn"
      },
      "source": [
        "First, we'll programmatically get the filename we just downloaded and then view the file via `xarray`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h40-TZA13rHn"
      },
      "outputs": [],
      "source": [
        "ds_PIXCVEC = xr.open_mfdataset(\"data_downloads/SWOT_L2_HR_PIXCVec_*.nc\", decode_cf=False,  engine='h5netcdf')\n",
        "ds_PIXCVEC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btcux1t93rHn"
      },
      "source": [
        "#### Simple plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZWenCYj3rHn"
      },
      "outputs": [],
      "source": [
        "pixcvec_htvals = ds_PIXCVEC.height_vectorproc.compute()\n",
        "pixcvec_latvals = ds_PIXCVEC.latitude_vectorproc.compute()\n",
        "pixcvec_lonvals = ds_PIXCVEC.longitude_vectorproc.compute()\n",
        "\n",
        "#Before plotting, we set all fill values to nan so that the graph shows up better spatially\n",
        "pixcvec_htvals[pixcvec_htvals > 15000] = np.nan\n",
        "pixcvec_latvals[pixcvec_latvals < 1] = np.nan\n",
        "pixcvec_lonvals[pixcvec_lonvals > -1] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vmin, vmax = np.nanpercentile(pixcvec_htvals, [2, 98])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "cax = ax.scatter(\n",
        "    pixcvec_lonvals,\n",
        "    pixcvec_latvals,\n",
        "    c=pixcvec_htvals,\n",
        "    s=1,\n",
        "    vmin=vmin,\n",
        "    vmax=vmax,\n",
        "    cmap='viridis',\n",
        "    rasterized=True\n",
        ")\n",
        "\n",
        "cbar = fig.colorbar(cax, ax=ax, shrink=0.5)\n",
        "cbar.set_label('Height (m)')\n",
        "\n",
        "cx.add_basemap(ax, crs='EPSG:4326', source=cx.providers.Esri.WorldStreetMap)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D8wRKWQ-JkOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfY8ht273rHn"
      },
      "source": [
        "#### **5. Raster NetCDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51PSnIrO3rHo"
      },
      "source": [
        "#### Search for data of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6EWLsgY3rHo"
      },
      "outputs": [],
      "source": [
        "raster_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_Raster_D', # enter 'SWOT_L2_HR_Raster_2.0' for version C (data before May 6, 2025)\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # can also specify by time\n",
        "                                        #bounding_box = (-72.73,46.58,-72.60,46.62), # filter by bounding box, to find your bounding box : http://bboxfinder.com/\n",
        "                                        granule_name = '*100m*_214_037F*') # here we filter by files with '100m' in the name (This collection has two resolution options: 100m & 250m)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B43pLnhHC5z"
      },
      "outputs": [],
      "source": [
        "#Print the granule characteristics associated with the selected pass, scene and resolution.\n",
        "print(raster_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCOLktF73rHo"
      },
      "source": [
        "Let's download one data file.  Here, we download the most recent file from the collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgL6WGbE3rHo"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([raster_results[-1]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuvxbYkj3rHo"
      },
      "source": [
        "#### Open data with xarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mm9T2Zv3rHo"
      },
      "source": [
        "First, we'll programmatically get the filename we just downloaded and then view the file via `xarray`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsOMnsqW3rHo"
      },
      "outputs": [],
      "source": [
        "ds_raster = xr.open_mfdataset(f'data_downloads/SWOT_L2_HR_Raster*', engine='h5netcdf')\n",
        "ds_raster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIoYfcIt3rHo"
      },
      "source": [
        "#### Quick interactive plot with `hvplot`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USz9iQ493rHo"
      },
      "outputs": [],
      "source": [
        "hv.extension('bokeh', 'matplotlib')\n",
        "plot = ds_raster['wse'].hvplot.image(y='y', x='x')\n",
        "hv.output(plot)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlNpHaxlPFXX"
      },
      "source": [
        "#### Mask a variable based on its quality indicator\n",
        "Example for an L2_HR_Raster dataset, indicator \"wse_qual\":\\\n",
        "0 = good\\\n",
        "1 = suspect -  may have large errors\\\n",
        "2 = degraded - very likely do have large errors\\\n",
        "3 = bad -  may be nonsensical and should be ignored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsVbk_B-RpZ3"
      },
      "outputs": [],
      "source": [
        "variable_to_mask = ds_raster['wse']\n",
        "mask_variable = ds_raster['wse_qual']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCX8XB1RV5rd"
      },
      "outputs": [],
      "source": [
        "# Set the condition to hide data based on the quality indicator\n",
        "mask_condition = mask_variable < 3\n",
        "masked_variable = variable_to_mask.where(mask_condition)\n",
        "masked_variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFJ0Gf1TWpWR"
      },
      "outputs": [],
      "source": [
        "# Update the masked variable in the dataset\n",
        "hv.extension('bokeh', 'matplotlib')\n",
        "plot2 = masked_variable.hvplot.image(y='y', x='x')\n",
        "hv.output(plot2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDn9o1htnTaB"
      },
      "source": [
        "#### Clip masked Raster NetCDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o4jcHoCRGDR"
      },
      "outputs": [],
      "source": [
        "# Create a file for clipped data\n",
        "os.makedirs('./content/clip_data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfdsxxFBCsxM"
      },
      "outputs": [],
      "source": [
        "#Define region of interest\n",
        "from shapely.geometry import box\n",
        "\n",
        "ROI = box(-67.28,45.72,-66.22,46.20)\n",
        "bbox_gdf = gpd.GeoDataFrame({'geometry': [ROI]}, crs='EPSG:4326')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFX2Yy1-nZKY"
      },
      "outputs": [],
      "source": [
        "# Set the spatial dimensions of the dataset and the coordinate reference system (CRS)\n",
        "masked_variable.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=True)\n",
        "masked_variable.rio.write_crs(\"epsg:32618\", inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qkm1TbdoKzR"
      },
      "outputs": [],
      "source": [
        "# Clip the raster\n",
        "# If needed, first reproject the region of interest to the same EPSG as the netCDF\n",
        "bbox_gdf = bbox_gdf.to_crs(\"epsg:32619\")\n",
        "clipped = masked_variable.rio.clip(bbox_gdf.geometry.apply(mapping), drop=True)\n",
        "\n",
        "\n",
        "hv.extension('bokeh', 'matplotlib')\n",
        "plot2 = clipped.hvplot.image(y='y', x='x')\n",
        "hv.output(plot2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Otx9IbRsiD7"
      },
      "outputs": [],
      "source": [
        "# Delete 'grid_mapping' attribute\n",
        "if 'grid_mapping' in clipped.attrs:\n",
        "    del clipped.attrs['grid_mapping']\n",
        "\n",
        "# Save clipped raster in a NetCDF file\n",
        "clipped_path = './content/clip_data/clipped_raster.nc'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNw1ksdCoNu2"
      },
      "source": [
        "#### **6. Working with HYDROCON - Rivers**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lP3hh9GY6WI"
      },
      "source": [
        "Extract time series with Hydrocon for the Reach and Node of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucPckRl57aR9"
      },
      "outputs": [],
      "source": [
        "from ast import And\n",
        "import folium\n",
        "import requests\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx2hHQkCoMvF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# You can choose a Node or Reach ID from the RiverSP product obtained in Section 1 or go here: https://www.swordexplorer.com/\n",
        "# Caution: Node and Reach IDs are not the same depending on the downloaded version (C or D).\n",
        "# Example for the Saint-Maurice River\n",
        "\n",
        "\n",
        "#feature='Node'\n",
        "#feature_id=\"71250300150401\"\n",
        "feature=\"Reach\"\n",
        "feature_id=\"72608300043\"\n",
        "start_time=\"2023-08-01T00:00:00Z\"\n",
        "end_time=\"2025-05-05T00:00:00Z\"\n",
        "collection_name = \"SWOT_L2_HR_RiverSP_D\"\n",
        "#fields=reach_id,time_str,wse,width\n",
        "\n",
        "parameters = (\n",
        "    \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?\"\n",
        "    + \"feature=\" + feature\n",
        "    + \"&feature_id=\" + feature_id\n",
        "    + \"&start_time=\" + start_time\n",
        "    + \"&end_time=\" + end_time\n",
        "    + \"&collection_name=\" + collection_name\n",
        "    + \"&output=geojson\"\n",
        "    + \"&fields=reach_id,time_str,wse,width,cycle_id\"\n",
        ")\n",
        "\n",
        "hydrocron_response = requests.get(\n",
        "    parameters\n",
        ").json()\n",
        "\n",
        "hydrocron_response\n",
        "\n",
        "# Extract the GeoJSON to display it on the map\n",
        "\n",
        "geojson_data = hydrocron_response['results']['geojson']\n",
        "\n",
        "geojson_data\n",
        "\n",
        "# Set up the map using Folium (https://python-visualization.github.io/folium/latest/)\n",
        "\n",
        "\n",
        "map = folium.Map (zoom_start=13, tiles=\"cartodbpositron\", width=700, height=700)\n",
        "\n",
        "\n",
        "folium.GeoJson(geojson_data, name='SWOT River Reach').add_to(map)\n",
        "folium.LayerControl().add_to(map)\n",
        "\n",
        "\n",
        "map.fit_bounds(map.get_bounds(), padding=(5, 5))\n",
        "\n",
        "map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HPkZ9hgZJtw"
      },
      "source": [
        "Display a water level time series for a node of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJdeAj8RZOzQ"
      },
      "outputs": [],
      "source": [
        "#St Jonh River\n",
        "\n",
        "feature='Node'\n",
        "feature_id=\"72608300050213\" # near Fredericton\n",
        "start_time=\"2023-08-01T00:00:00Z\"\n",
        "end_time=\"2026-01-21T00:00:00Z\"\n",
        "collection_name = \"SWOT_L2_HR_RiverSP_D\"\n",
        "#fields=reach_id,time_str,wse,width\n",
        "\n",
        "#parameters = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?feature=\"+feature+\"&feature_id=\"+ feature_id +\"&start_time=2024-01-01T00:00:00Z&end_time=2024-06-14T00:00:00Z&output=csv&fields=reach_id,node_id,time_str,node_q,wse,width,cycle_id\"\n",
        "\n",
        "parameters = (\n",
        "    \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?\"\n",
        "    + \"feature=\" + feature\n",
        "    + \"&feature_id=\" + feature_id\n",
        "    + \"&start_time=\" + start_time\n",
        "    + \"&end_time=\" + end_time\n",
        "    + \"&collection_name=\" + collection_name\n",
        "    + \"&output=csv\"\n",
        "    + \"&fields=reach_id,node_id,time_str,node_q,wse,width,cycle_id\"\n",
        ")\n",
        "\n",
        "hydrocron_response = requests.get(\n",
        "    parameters\n",
        ").json()\n",
        "\n",
        "hydrocron_response\n",
        "csv_str = hydrocron_response['results']['csv']\n",
        "df = pd.read_csv(StringIO(csv_str))\n",
        "ind = df.node_q<3\n",
        "\n",
        "df = df[df['time_str'] != 'no_data']\n",
        "df.time_str = pd.to_datetime(df.time_str, format='%Y-%m-%dT%H:%M:%SZ')\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "plt.plot(df.time_str[ind], df.wse[ind], marker='o', linestyle='None')\n",
        "\n",
        "plt.ylabel('Water surface elevation (m)')\n",
        "plt.xlabel('SWOT observation date')\n",
        "plt.title('Water Surface Elevation from Hydrocron for Node: ' + str(df.node_id[0]))\n",
        "\n",
        "#Save image\n",
        "plt.savefig('/content/WSE_Hydrocon_river.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoVqg9FPLGVp"
      },
      "source": [
        "#### Extract profile between two nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUcX173DLLc2"
      },
      "outputs": [],
      "source": [
        "# Create a file for the data\n",
        "os.makedirs('./content/profil_node', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zf75nD8Rv3U"
      },
      "source": [
        "Data search and download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R1k0Va5MYjZ"
      },
      "outputs": [],
      "source": [
        "# Function to search for and download the data\n",
        "def download_data(pass_numbers, continent_code, path, temporal_range):\n",
        "    links_list = []\n",
        "    for pass_num in pass_numbers:\n",
        "        river_results = earthaccess.search_data(\n",
        "            short_name='SWOT_L2_HR_RIVERSP_D',\n",
        "            temporal=temporal_range,\n",
        "            granule_name=f\"*Node*_{pass_num}_{continent_code}*\"\n",
        "        )\n",
        "        links_list.extend([earthaccess.results.DataGranule.data_links(result, access='external')[0]\n",
        "                           for result in river_results])\n",
        "\n",
        "    earthaccess.download(links_list, path)\n",
        "    return links_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q92l9TUR1Is"
      },
      "source": [
        "Extract ZIPs and load shapefiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VKLRwl8Mm6C"
      },
      "outputs": [],
      "source": [
        "# Founction to extract ZIPs\n",
        "def extract_files(links_list, path):\n",
        "    filenames = [link.split(\"/\")[-1] for link in links_list]\n",
        "    for filename in filenames:\n",
        "        with zipfile.ZipFile(f\"{path}/{filename}\", 'r') as zip_ref:\n",
        "            zip_ref.extractall(path)\n",
        "    return filenames\n",
        "\n",
        "# Founction to load shapefiles\n",
        "def load_shapefiles(filenames, path):\n",
        "    filename_shps = [filename.replace('zip', 'shp') for filename in filenames]\n",
        "    return gpd.GeoDataFrame(pd.concat([gpd.read_file(f\"{path}/{shp}\") for shp in filename_shps], ignore_index=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lexmm0eLR7l-"
      },
      "source": [
        "Node filtering and distance calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e-y_bc_MsGI"
      },
      "outputs": [],
      "source": [
        "# Founction for node filtering\n",
        "def filter_data_by_nodes(SWOT_HR_df, up_node, dn_node, date=None):\n",
        "    filtered = SWOT_HR_df[\n",
        "        (SWOT_HR_df['node_id'] >= dn_node) &\n",
        "        (SWOT_HR_df['node_id'] < up_node) &\n",
        "        (SWOT_HR_df['wse'] != -999999999999)\n",
        "    ] #\n",
        "    if date:\n",
        "        filtered = filtered[filtered['time_str'].str.contains(date)]\n",
        "    return filtered.sort_values(['node_id'])\n",
        "\n",
        "# Founction for distance calculation\n",
        "\n",
        "def calculate_distances(SWOT_HR_profil):\n",
        "    \"\"\"\n",
        "    Calculates the cumulative distance from the 'p_length' column of SWOT_HR_profil_1.\n",
        "\n",
        "    \"\"\"\n",
        "    delta = SWOT_HR_profil.p_length\n",
        "    # Initialize a zero array to store cumulative distances\n",
        "    dist_1=np.zeros((len(SWOT_HR_profil),1))\n",
        "\n",
        "    # Cumulative distance calculation\n",
        "    for n in range(len(SWOT_HR_profil)-1):\n",
        "      dist_1[n+1]=dist_1[n]+delta.iloc[n]\n",
        "\n",
        "    return dist_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XheFxi3MSAKF"
      },
      "source": [
        "Profile plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v59FAcrHM4us"
      },
      "outputs": [],
      "source": [
        "# Founction to plot the profile\n",
        "def plot_profile(profile, dist, label):\n",
        "    fig, ax = plt.subplots(figsize=(15, 5))\n",
        "    ax.plot(dist, profile['wse'], marker='o', linestyle='None', label=label)\n",
        "    ax.set_xlabel('Cumulative distance (m)')\n",
        "    ax.set_ylabel('Water surface elevation (wse)')\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "    #Save image\n",
        "    plt.savefig('/content/profil.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqzSg69hSFGZ"
      },
      "source": [
        "Variables to modify as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hj2QreNNdGZ"
      },
      "outputs": [],
      "source": [
        "# Directory path where the data will be saved and variables to identify\n",
        "path = '/content/profil_node'\n",
        "pass_number    = [\"214\"]  # pass number\n",
        "continent_code = \"NA\"  # Continent code (NA for North America)\n",
        "temporal_range = '2025-05-24 00:00:00', '2025-05-27 23:59:59'\n",
        "dn_node = 72608300270021 # Downstream node ID ex: Nashwaak River\n",
        "up_node = 72608300280071 # Upstream node ID ex : Nashwaak River"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjNLJ5ePSNEN"
      },
      "source": [
        "Complete execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA_Ka61WSzL8"
      },
      "outputs": [],
      "source": [
        "# Data dowload\n",
        "links_list = download_data(pass_number, continent_code, path, temporal_range)\n",
        "filenames = extract_files(links_list, path)\n",
        "\n",
        "# Load shapefiles\n",
        "SWOT_HR_df = load_shapefiles(filenames, path)\n",
        "SWOT_HR_df['node_id'] = SWOT_HR_df['node_id'].astype(float)\n",
        "\n",
        "\n",
        "# Filter the profile for May 26, 2025\n",
        "profile_1 = filter_data_by_nodes(SWOT_HR_df, up_node, dn_node, date='2025-05-26')\n",
        "\n",
        "# Calculate cumulative distances\n",
        "dist_1 = calculate_distances(profile_1)\n",
        "\n",
        "# Plot profil\n",
        "plot_profile(profile_1, dist_1, 'May 26, 2025')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6WkW4Da13G5"
      },
      "source": [
        "#### **7. Working with HYDROCON - Lake**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc6GZFiw5w6l"
      },
      "source": [
        "NOTE: Due to the size of the original polygon (L2_HR_LakeSP), only the lake's central point is returned. This is intended to facilitate compliance with GeoJSON specifications. The positions of the central points should not be considered exact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ7xYKHR2Oe3"
      },
      "outputs": [],
      "source": [
        "feature=\"PriorLake\"\n",
        "feature_id=\"7260055912\" #oromocto lake\n",
        "start_time=\"2025-05-06T00:00:00Z\"\n",
        "end_time=\"2025-10-12T00:00:00Z\"\n",
        "collection_name = \"SWOT_L2_HR_LakeSP_D\"  # specify the desired version\n",
        "\n",
        "parameters = (\n",
        "    \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?\"\n",
        "    f\"feature={feature}&feature_id={feature_id}\"\n",
        "    f\"&start_time={start_time}&end_time={end_time}\"\n",
        "    f\"&collection_name={collection_name}\"\n",
        "    \"&output=geojson&fields=lake_id,time_str,wse,area_total\"\n",
        ")\n",
        "\n",
        "hydrocron_response = requests.get(\n",
        "    parameters\n",
        ").json()\n",
        "\n",
        "hydrocron_response\n",
        "\n",
        "# Extract GeoJSON for map display\n",
        "\n",
        "geojson_data = hydrocron_response['results']['geojson']\n",
        "\n",
        "geojson_data\n",
        "\n",
        "valid_features = [\n",
        "    feature for feature in geojson_data['features']\n",
        "    if float(feature['properties']['wse']) > 0 and  # Select valid WSE values\n",
        "       -90 <= feature['geometry']['coordinates'][1] <= 90 and  # Select valid latitude values\n",
        "       -180 <= feature['geometry']['coordinates'][0] <= 180  # Select valid longitude values\n",
        "]\n",
        "\n",
        "# Create a geojson with valid features\n",
        "filtered_geojson = {\n",
        "    'type': 'FeatureCollection',\n",
        "    'features': valid_features\n",
        "}\n",
        "\n",
        "\n",
        "filtered_geojson\n",
        "\n",
        "\n",
        "# Set up the map using Folium (https://python-visualization.github.io/folium/latest/)\n",
        "\n",
        "map = folium.Map (tiles=\"cartodbpositron\", width=700, height=700)\n",
        "\n",
        "# Add the GeoJSON from Hydrocron to the map\n",
        "folium.GeoJson(filtered_geojson, name='SWOT Prior Lake').add_to(map)\n",
        "folium.LayerControl().add_to(map)\n",
        "\n",
        "\n",
        "# Center on the river\n",
        "map.fit_bounds(map.get_bounds(), padding=(5, 5))\n",
        "\n",
        "map\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOodWeIU4xYI"
      },
      "source": [
        "Show water level time series for a node of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_aNk0JcKv0-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from io import StringIO\n",
        "\n",
        "# Parameters definition\n",
        "feature = \"PriorLake\"\n",
        "feature_id = \"7260055912\"\n",
        "start_time = \"2023-08-31T00:00:00Z\"\n",
        "end_time = \"2026-01-21T00:00:00Z\"\n",
        "collection_name = \"SWOT_L2_HR_LakeSP_D\"\n",
        "parameters = (\n",
        "    \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?\"\n",
        "    + \"feature=\" + feature\n",
        "    + \"&feature_id=\" + feature_id\n",
        "    + \"&start_time=\" + start_time\n",
        "    + \"&end_time=\" + end_time\n",
        "    + \"&collection_name=\" + collection_name\n",
        "    + \"&output=csv\"\n",
        "    + \"&fields=lake_id,time_str,wse,area_total,quality_f,dark_frac\"\n",
        ")\n",
        "\n",
        "\n",
        "hydrocron_response = requests.get(parameters).json()\n",
        "\n",
        "# Extrct CSV and create DataFrame\n",
        "csv_str = hydrocron_response['results']['csv']\n",
        "df = pd.read_csv(StringIO(csv_str))\n",
        "\n",
        "# Select data where 'time_str' is not equal to 'no_data'\n",
        "df = df[df['time_str'] != 'no_data']\n",
        "# Select data where 'quality_f' is equal to 0 (good)\n",
        "df = df[df['quality_f'] == 0]\n",
        "# Select data where 'dark_frac' is less than 50%\n",
        "df = df[df['dark_frac'] < 0.5]\n",
        "# Convert 'time_str' to datetime format\n",
        "df['time_str'] = pd.to_datetime(df['time_str'], format='%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "# Create figure\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "plt.plot(df['time_str'], df['wse'], marker='o', linestyle='None')\n",
        "\n",
        "\n",
        "plt.ylabel('Water surface elevation (m)')\n",
        "plt.xlabel('SWOT observation date')\n",
        "plt.title('Water Surface Elevation from Hydrocron for Lake: ' + str(df['lake_id'].iloc[0]))\n",
        "\n",
        "# Save image\n",
        "plt.savefig('/content/WSE_Hydrocron_Lake.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDK9NcPlDM3B"
      },
      "source": [
        "#### **8. Transform reference systems with csrsppy_modified**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgpGFfQNDh0B"
      },
      "source": [
        "The SWOT reference system is not the same as Canada's. It is therefore necessary to convert the data.\n",
        "**CAUTION: Each Canadian provincial geodetic agency may use a different vertical reference system and epoch!**\n",
        "In this section, the transformation is performed to the vertical reference system of New Brunswick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTlMR_cyDsKJ"
      },
      "source": [
        "Clip the data using your area of interest (necessary because each province may adopt a different epoch and vertical reference system).\n",
        "\n",
        "**Change the RiverSP file name to match the one you want and the data acquisition date.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAAqbfFIDz9K"
      },
      "outputs": [],
      "source": [
        "# Download the shapefile to clip\n",
        "\n",
        "shapefile_path = '/content/data_downloads/SWOT_L2_HR_RiverSP_Node_044_214_NA_20260111T003728_20260111T004317_PID0_01.zip'\n",
        "\n",
        "points_gdf = gpd.read_file(shapefile_path)\n",
        "\n",
        "# Define bbox\n",
        "bounding_box = box(-67.28,45.72,-66.22,46.20)\n",
        "\n",
        "# Filtrer les points qui sont à l'intérieur du bbox\n",
        "gdf_subset = points_gdf[points_gdf.geometry.within(bounding_box)].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93L7HHUjLR-1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Parameter to change\n",
        "\n",
        "acquisition_date = \"2026-01-11\"  # Enter the date of acquisition of the data to be converted here to ensure the correct epoch.\n",
        "\n",
        "# Convert the date decimal year\n",
        "def decimal_year(dt):\n",
        "    year_start = datetime(dt.year, 1, 1)\n",
        "    year_end = datetime(dt.year + 1, 1, 1)\n",
        "    year_length = (year_end - year_start).total_seconds()\n",
        "    seconds_passed = (dt - year_start).total_seconds()\n",
        "    return dt.year + seconds_passed / year_length\n",
        "\n",
        "s_epoch = decimal_year(datetime.strptime(acquisition_date, \"%Y-%m-%d\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlSWlUBeRWTx"
      },
      "outputs": [],
      "source": [
        "# Use the filtered GeoDataFrame\n",
        "gdf_nodes = gdf_subset.copy()\n",
        "\n",
        "# Download or synchronize missing grids\n",
        "sync_missing_grid_files()\n",
        "\n",
        "# Create column 'h' = WSE_SWOT + geoid_hght\n",
        "gdf_nodes['h'] = gdf_nodes['wse'] + gdf_nodes['geoid_hght']\n",
        "\n",
        "#  Prepare the list of results\n",
        "transformed_results = [None] * len(gdf_nodes)\n",
        "\n",
        "# Transformation\n",
        "for idx, row in enumerate(tqdm(gdf_nodes.itertuples(), total=len(gdf_nodes), desc=\"Transformation\")):\n",
        "\n",
        "    transformer = CSRSTransformer(\n",
        "        t_ref_frame=Reference.NAD83CSRS,\n",
        "        s_ref_frame=Reference.ITRF14,\n",
        "        s_coords=CoordType.GEOG,\n",
        "        t_coords=CoordType.GEOG,\n",
        "        s_epoch=s_epoch,\n",
        "        t_epoch=2010,\n",
        "        epoch_shift_grid='ca_nrc_NAD83v70VG.tif',\n",
        "        s_vd=VerticalDatum.WGS84,\n",
        "        t_vd=VerticalDatum.CGG2013\n",
        "    )\n",
        "    # Source coordinates\n",
        "    coords = [(row.lon, row.lat, row.h)]\n",
        "\n",
        "    # Transformation (convert map to list)\n",
        "    out_coords = list(transformer(coords))\n",
        "\n",
        "    transformed_results[idx] = out_coords[0][2]\n",
        "\n",
        "# Add the transformed column to the GeoDataFrame\n",
        "gdf_nodes['wse_transformed'] = transformed_results\n",
        "\n",
        "# Create the folder\n",
        "output_dir = '/content/ref_change'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save shapefile\n",
        "gdf_nodes.to_file(os.path.join(output_dir, 'nodes_transformed.shp'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Munla4LN81su"
      },
      "source": [
        "#### **9. Download all your data to your local computer at once**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF1okpJjGAny"
      },
      "source": [
        "#### You can download data one by one by clicking on the three small dots to the right of the file name and then on “Download”. To download all the data at once, run the following cells. Execution and downlaod times may be long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7fiZ1tk-uSZ"
      },
      "outputs": [],
      "source": [
        "# Create a zip file with all data and choose the file to compress\n",
        "!zip -r /content/data.zip /content/data_downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04WSzmVY-5gz"
      },
      "outputs": [],
      "source": [
        "# Download data zip file\n",
        "from google.colab import files\n",
        "files.download(\"/content/data.zip\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5a4443810289f87e0f862ef34d31d94a0884467de587e41820bef73e0713c5c1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}