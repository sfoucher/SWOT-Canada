{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b11PMt8r3rHd"
      },
      "source": [
        "![](https://img.shields.io/badge/PO.DAAC-Contribution-%20?color=grey&labelColor=blue)\n",
        "\n",
        "> Matériel provenant du PO.DAAC Cookbook, cliquez sur [ce lien](https://github.com/podaac/tutorials/blob/master/notebooks/datasets/SWOTHR_localmachine.ipynb) pour accéder à la version GitHub du carnet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNpTEGt3rHe"
      },
      "source": [
        "# Exploration des données hydrologiques SWOT à partir d'une machine locale\n",
        "\n",
        "## Accès et visualisation des ensembles de données SWOT\n",
        "\n",
        "### Prérequis:\n",
        "Environnement de calcul local, par exemple un ordinateur portable.\n",
        "\n",
        "### Objectifs d'apprentissage:\n",
        "- Accéder aux données SWOT HR archivés dans le nuage NASA Earthdata en les téléchargeant sur une machine locale.\n",
        "- Visualiser les données pour une vérification rapide.\n",
        "\n",
        "#### SWOT Level 2 KaRIn High Rate Version 2.0 Datasets:\n",
        "\n",
        "1. **River Vector Shapefile** - SWOT_L2_HR_RIVERSP_2.0\n",
        "\n",
        "2. **Lake Vector Shapefile** - SWOT_L2_HR_LAKESP_2.0\n",
        "\n",
        "3. **Water Mask Pixel Cloud NetCDF** - SWOT_L2_HR_PIXC_2.0\n",
        "\n",
        "4. **Water Mask Pixel Cloud Vector Attribute NetCDF** - SWOT_L2_HR_PIXCVec_2.0\n",
        "\n",
        "5. **Raster NetCDF** - SWOT_L2_HR_Raster_2.0\n",
        "\n",
        "6. **Single Look Complex Data product** - SWOT_L1B_HR_SLC_2.0\n",
        "\n",
        "_Ce carnet a été modifié et traduit par l'équipe de l'Université de Sherbrooke et de l'Université Laval. Auteurs originaux :  Cassie Nickles, NASA PO.DAAC (Feb 2024) || Autres contributeurs : Zoe Walschots (PO.DAAC Summer Intern 2023), Catalina Taglialatela (NASA PO.DAAC), Luis Lopez (NASA NSIDC DAAC)*._\n",
        "\n",
        "_Dernière mise à jour : 25 octobre 2024_\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60WhXudr3rHe"
      },
      "source": [
        "### Librairies nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iUtXlwn3rHe"
      },
      "outputs": [],
      "source": [
        "!pip install contextily\n",
        "!pip install earthaccess\n",
        "!pip install --upgrade holoviews hvplot\n",
        "!pip install holoviews hvplot bokeh xarray\n",
        "!pip install rioxarray\n",
        "!pip install rasterio\n",
        "!pip install shapely\n",
        "!pip install geoviews\n",
        "!pip install pyproj\n",
        "\n",
        "#!pip install hvplot\n",
        "\n",
        "\n",
        "import glob\n",
        "import h5netcdf\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import contextily as cx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import hvplot.xarray\n",
        "import holoviews as hv\n",
        "import zipfile\n",
        "import earthaccess\n",
        "import os\n",
        "import rioxarray\n",
        "from shapely.geometry import mapping\n",
        "from shapely.geometry import Point\n",
        "import csv\n",
        "import shapefile\n",
        "import geoviews as gvts\n",
        "from pyproj import Proj\n",
        "import logging\n",
        "from shapely.geometry import box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUjA7H1Y3rHf"
      },
      "source": [
        "### Connexion à Earthdata\n",
        "\n",
        "Un compte Earthdata est nécessaire pour accèder aux données du système Earthdata de la NASA. Si vous n'en avez pas encore, rendez-vous sur https://urs.earthdata.nasa.gov pour créer votre compte. La création du compte est gratuite et ne prend que quelques minutes. Nous utilisons `earthaccess` pour vous identifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqbEXA7x3rHf"
      },
      "outputs": [],
      "source": [
        "auth = earthaccess.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4soPmS-Jbd"
      },
      "source": [
        "### Accès à un fichier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEcoud9p3rHf"
      },
      "source": [
        "\n",
        "#### **1. River Vector Shapefiles**\n",
        "\n",
        "Le lien d'accès https peut être trouvé en utilisant earthaccess data search. Cette collection est composée de fichiers Reach et Node.\n",
        "\n",
        "Earthdata Search [(voir le tutoriel)](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/tutorials/01_Earthdata_Search.html) peut également être utilisé pour effectuer une recherche manuelle à partir d'une interface graphique.\n",
        "\n",
        "Pour des conseils supplémentaires concernant la recherche spatiale des données SWOT HR L2, voir également [PO.DAAC Cookbook - SWOT Chapter tips section](https://podaac.github.io/tutorials/quarto_text/SWOT.html#tips-for-swot-hr-spatial-search).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RoIlpJpigZ-"
      },
      "source": [
        "#### Recherche des données d'intérêt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwp1-ZHV3rHg"
      },
      "outputs": [],
      "source": [
        "#Recupération des granules ayant les caractéristiques voulues en passant par la fonction `earthdata.search_data`\n",
        "river_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_RIVERSP_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # peut également être filtré en fonction de la plage temporelle\n",
        "                                        granule_name = '*Node*_576_NA*') # ici nous filtrons par fichiers Nodes (pas Reach), par passe et par continent\n",
        "                                                                         # spécifier 'Node' ou 'Reach' selon le fichier voulu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP4AgA4x0KAK"
      },
      "outputs": [],
      "source": [
        "#Imprimer les caractéristiques des granules associés à la passe sélectionnée.\n",
        "print(river_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGGZ2Jxu3rHg"
      },
      "source": [
        "#### Téléchargez, décompressez et lisez les données\n",
        "\n",
        "Télédéchargeons le fichier de données sélectionné ! `earthaccess.download` a une liste comme format d'entrée, nous devons donc mettre des crochets autour du fichier que nous voulons.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz2o8DL83rHg"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([river_results[3]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr6Tvrdb3rHg"
      },
      "source": [
        "Le format natif de ces données est un fichier .zip et nous voulons le fichier .shp dans ce fichier .zip. Nous devons donc extraire les données pour les ouvrir. Nous allons récupérer le nom du fichier que nous venons de télécharger, puis extraire toutes les données dans le dossier `data_downloads`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed_mTohH3rHg"
      },
      "outputs": [],
      "source": [
        "filename = earthaccess.results.DataGranule.data_links(river_results[3], access='external')\n",
        "filename = filename[0].split(\"/\")[-1]\n",
        "filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW2hQ-CF3rHg"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(f'data_downloads/{filename}', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_downloads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE5Ge14q3rHh"
      },
      "source": [
        "Ouverture du fichier shapefile avec `geopandas`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6RwizoQ3rHh"
      },
      "outputs": [],
      "source": [
        "filename_shp = filename.replace('.zip','.shp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5dZOp-O3rHh"
      },
      "outputs": [],
      "source": [
        "SWOT_HR_shp1 = gpd.read_file(f'data_downloads/{filename_shp}')\n",
        "\n",
        "#afficher la table d'attributs\n",
        "SWOT_HR_shp1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJHd9KST3rHh"
      },
      "source": [
        "#### Affichage des données des rivières SWOT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher les élévations de l'eau (WSE, Water Surface Elevation)"
      ],
      "metadata": {
        "id": "sr7C9k3PSf__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir les coordonnées de la boîte englobante (xmin, ymin, xmax, ymax)\n",
        "bounding_box = box(-72.9, 46.34, -72.5, 46.87)\n",
        "\n",
        "# Filtrer les valeurs aberrantes (-999999999999) dans la colonne 'wse'\n",
        "SWOT_HR_shp1_filtered = SWOT_HR_shp1[SWOT_HR_shp1['wse'] != -999999999999]\n",
        "\n",
        "# Découper le shapefile avec la boîte englobante\n",
        "SWOT_HR_shp1_clipped = gpd.clip(SWOT_HR_shp1_filtered, bounding_box)\n",
        "\n",
        "# Affichage avec WSE après découpage et filtrage des valeurs aberrantes\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# Tracer la figure avec des couleurs basées sur la colonne WSE\n",
        "SWOT_HR_shp1_clipped.plot(ax=ax, column='wse', cmap='PuBuGn', legend=True,\n",
        "                          legend_kwds={'label': \"Niveau de l'élévation de l'eau (WSE)\", 'orientation': \"vertical\"})\n",
        "\n",
        "# Ajouter une carte de base\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp1_clipped.crs, source=cx.providers.Esri.WorldStreetMap)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4TZRnkq7Rf6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher la qualité des noeuds"
      ],
      "metadata": {
        "id": "I6VyAS5CSst4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.colors as mcolors\n",
        "# Créer une liste des valeurs uniques dans node_q\n",
        "unique_values = np.sort(SWOT_HR_shp1['node_q'].unique())\n",
        "\n",
        "# Créer un colormap discret\n",
        "cmap = plt.get_cmap('RdYlGn_r', len(unique_values))  # choisir un colormap avec des couleurs distinctes\n",
        "norm = mcolors.BoundaryNorm(boundaries=np.arange(len(unique_values)+1)-0.5, ncolors=len(unique_values))\n",
        "\n",
        "# Affichage avec coloration basée sur node_q\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# Tracer la figure avec les couleurs selon node_q\n",
        "SWOT_HR_shp1.plot(ax=ax, column='node_q', cmap=cmap, norm=norm, legend=False)\n",
        "\n",
        "# Ajouter une légende discrète\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "sm.set_array([])  # pas nécessaire de définir les données pour ce cas\n",
        "\n",
        "# Afficher uniquement les valeurs uniques de node_q dans la légende\n",
        "cbar = plt.colorbar(sm, ax=ax, ticks=range(len(unique_values)))\n",
        "cbar.ax.set_yticklabels(unique_values)\n",
        "cbar.set_label('Qualité des nodes')\n",
        "\n",
        "# Définir les limites de la zone d'intérêt\n",
        "ax.set_ylim(46.34, 46.87)\n",
        "ax.set_xlim(-72.9, -72.5)\n",
        "\n",
        "\n",
        "# Ajouter une carte de base\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp1.crs, source=cx.providers.Esri.WorldStreetMap)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"\"\"0 = bon\n",
        "1 = suspect - peut comporter des erreurs importantes\n",
        "2 = dégradé - susceptible de présenter des erreurs importantes\n",
        "3 = mauvais - peut être aberrant et doit être ignoré\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "muk55soidEct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DndxI6sA3rHh"
      },
      "outputs": [],
      "source": [
        "# Il est également possible d'afficher les données avec la fonction `explore` de geopandas\n",
        "#SWOT_HR_shp1.explore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "803Ughb13rHh"
      },
      "source": [
        "#### **2. Lake Vector Shapefiles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhrHLXfL3rHh"
      },
      "source": [
        "Les fichiers vectoriels des lacs sont accessibles de la même manière que les fichiers vectoriels des rivières ci-dessus.\n",
        "\n",
        "Pour des conseils supplémentaires sur la recherche spatiale des données SWOT HR L2, voir également [PO.DAAC Cookbook - SWOT Chapter tips section](https://podaac.github.io/tutorials/quarto_text/SWOT.html#tips-for-swot-hr-spatial-search)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTdxr0-43rHh"
      },
      "source": [
        "#### Recherche des données d'intérêt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlHMg6UO3rHh"
      },
      "outputs": [],
      "source": [
        "lake_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_LAKESP_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # peut également être filtré en fonction de la plage temporelle\n",
        "                                        granule_name = '*Prior*_576_NA*') # ici nous filtrons les fichiers avec 'Prior' (Cette collection a trois options : Obs, Unassigned, et Prior), par passe et par continent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJYhv_hb__7M"
      },
      "outputs": [],
      "source": [
        "#Imprimer les caractéristiques des granules associés à la passe sélectionnée.\n",
        "print(lake_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uay3D0DS3rHi"
      },
      "source": [
        "Téléchargez le fichier de données sélectionné ! earthaccess.download a une liste comme format d'entrée, nous devons donc mettre des crochets autour du fichier que nous voulons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WvT4rEh3rHi"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([lake_results[0]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "barmJ-eQ3rHi"
      },
      "source": [
        "Le format natif de ces données est un fichier .zip et nous voulons le fichier .shp dans ce fichier .zip. Nous devons donc extraire les données pour les ouvrir. Nous allons récupérer le nom du fichier que nous venons de télécharger, puis extraire toutes les données dans le dossier `data_downloads`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uHEgBWL3rHi"
      },
      "outputs": [],
      "source": [
        "filename2 = earthaccess.results.DataGranule.data_links(lake_results[0], access='external')\n",
        "filename2 = filename2[0].split(\"/\")[-1]\n",
        "filename2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL9U2jls3rHi"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(f'data_downloads/{filename2}', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data_downloads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx4N4fCr3rHi"
      },
      "source": [
        "Ouverture du shapefile avec `geopandas`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB0mMazr3rHi"
      },
      "outputs": [],
      "source": [
        "filename_shp2 = filename2.replace('.zip','.shp')\n",
        "filename_shp2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T03QYmUd3rHi"
      },
      "outputs": [],
      "source": [
        "SWOT_HR_shp2 = gpd.read_file(f'data_downloads/{filename_shp2}')\n",
        "\n",
        "#Afficher la table d'attributs\n",
        "SWOT_HR_shp2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbPu8_GA3rHi"
      },
      "source": [
        "#### Affichage des données des lacs SWOT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher les élévations de l'eau (WSE, Water Surface Elevation)"
      ],
      "metadata": {
        "id": "z0a6e0sIbZnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrer les valeurs aberrantes et définir la boîte englobante en une seule ligne\n",
        "SWOT_HR_shp2_clipped = gpd.clip(SWOT_HR_shp2.query('wse != -999999999999'), box(-73.10, 47.37, -73.02, 47.44))\n",
        "\n",
        "# Afficher le shapefile découpé avec la colonne WSE\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "\n",
        "\n",
        "# Tracer la figure avec des couleurs basées sur la colonne WSE\n",
        "SWOT_HR_shp2_clipped.plot(ax=ax, column='wse', cmap='PuBuGn', legend=True,\n",
        "                          legend_kwds={'label': \"Niveau de d'élévation de l'eau (WSE)\", 'orientation': \"vertical\"})\n",
        "\n",
        "\n",
        "# Ajouter les valeurs de WSE sur le shapefile\n",
        "centroids = SWOT_HR_shp2_clipped.geometry.centroid\n",
        "for x, y, label in zip(centroids.x, centroids.y, SWOT_HR_shp2_clipped['wse']):\n",
        "    ax.text(x, y, f'{label:.2f}', fontsize=8, ha='center', va='center', color='black')\n",
        "\n",
        "# Ajouter une carte de base\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp2_clipped.crs, source=cx.providers.Esri.WorldStreetMap)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Znw8nzQtdMvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher le pourcentage de *dark water*. (Variable dark_frac: Fraction de la surface totale du lac (area_total) couverte par du *dark water*, égale à 1-(area_detct/area_total). Cette valeur est comprise entre 0 et 1, 0 indiquant qu'il n'y a pas d'eau sombre et 1 indiquant qu'il y a 100 % d'eau sombre.)"
      ],
      "metadata": {
        "id": "CvI6sAs9bhnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrer les valeurs aberrantes et définir la boîte englobante\n",
        "SWOT_HR_shp2_clipped = gpd.clip(SWOT_HR_shp2.query('wse != -999999999999'), box(-73.10, 47.37, -73.02, 47.44))\n",
        "\n",
        "# Afficher le shapefile découpé avec la colonne WSE\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "\n",
        "\n",
        "# Tracer la figure avec des couleurs basées sur la colonne WSE\n",
        "SWOT_HR_shp2_clipped.plot(ax=ax, column='dark_frac', cmap='RdYlGn_r', legend=True,\n",
        "                          legend_kwds={'label': \"Pourcentage de dark water\", 'orientation': \"vertical\"})\n",
        "\n",
        "\n",
        "# Ajouter les valeurs de WSE sur le shapefile\n",
        "centroids = SWOT_HR_shp2_clipped.geometry.centroid\n",
        "for x, y, label in zip(centroids.x, centroids.y, SWOT_HR_shp2_clipped['dark_frac']):\n",
        "    ax.text(x, y, f'{label:.2f}', fontsize=8, ha='center', va='center', color='black')\n",
        "\n",
        "# Ajouter une carte de base\n",
        "cx.add_basemap(ax, crs=SWOT_HR_shp2_clipped.crs, source=cx.providers.Esri.WorldStreetMap)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mogzikk6fsHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8FCTuFj3rHi"
      },
      "source": [
        "L'accès aux fichiers ci-dessous est différent de celui des fichiers .shp présentés jusqu'à maintenant. Puisque les collections SWOT HR suivantes sont stockées sous forme de fichiers **netCDF** dans le nuage, nous n'avons pas besoin d'extraire les shapefiles d'un fichier zip. Pour le reste des produits, nous les ouvrirons via `xarray` et non `geopandas`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5VVWM3T3rHj"
      },
      "source": [
        "#### **3. Water Mask Pixel Cloud NetCDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyzubIWy3rHj"
      },
      "source": [
        "#### Recherche des données de la collection selon les caractéristiques voulues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI7j-xMH3rHj"
      },
      "outputs": [],
      "source": [
        "pixc_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_PIXC_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # filtre en fonction de la plage temporelle\n",
        "                                        granule_name = '*_576_073L*') # numéro de pass, de tuile et côté de fauchée (R ou L)\n",
        "                                        #bounding_box = (-72.73,46.58,-72.60,46.62)) # filtre par boîte englobante, pour trouver votre boîte englobante : http://bboxfinder.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo-3hTY45bdY"
      },
      "outputs": [],
      "source": [
        "#Imprimer les caractéristiques des granules associés à la passe, à la tuile et à la fauchée.\n",
        "print(pixc_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT6c7eod3rHj"
      },
      "source": [
        "Téléchargez le fichier de données sélectionné ! earthaccess.download a une liste comme format d'entrée, nous devons donc mettre des crochets autour du fichier que nous voulons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou7k7iha3rHj"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([pixc_results[1]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQE3zVRR3rHm"
      },
      "source": [
        "#### Ourvrir les données avec xarray\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gPsWBV_3rHm"
      },
      "source": [
        "Ces fichiers netCDF sont formatés en trois groupes intitulés \"pixel cloud\", \"tvp\", ou \"noise\" (plus de détails [ici](https://podaac-tools.jpl.nasa.gov/drive/files/misc/web/misc/swot_mission_docs/pdd/D-56411_SWOT_Product_Description_L2_HR_PIXC_20200810.pdf)). Afin d'accéder aux coordonnées et aux variables du fichier, un groupe doit être spécifié lors de l'appel à xarray open_dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKmO1lj63rHm"
      },
      "outputs": [],
      "source": [
        "ds_PIXC = xr.open_mfdataset(\"data_downloads/SWOT_L2_HR_PIXC_*.nc\", group = 'pixel_cloud', engine='h5netcdf') #Si plusieurs fichiers PIXC sont téléchargés dans le fichier data_download, spécifiez lequel des fichiers doit être affiché.\n",
        "ds_PIXC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM7lzUnr3rHm"
      },
      "source": [
        "#### Affichage des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOs0fhyv3rHm"
      },
      "outputs": [],
      "source": [
        "# L'affichage peut prendre quelques minutes (environ 5 minutes)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "cax=plt.scatter(x=ds_PIXC.longitude, y=ds_PIXC.latitude, c=ds_PIXC.height, s=1) #ajustement de la taille des pixels avec s= taille désirée\n",
        "cbar = fig.colorbar(cax, ax=ax, shrink=0.5)\n",
        "cbar.set_label('Height (m)')\n",
        "\n",
        "# ajustement de la barre de couleur\n",
        "cbar.ax.set_aspect('auto')\n",
        "\n",
        "cx.add_basemap(ax, crs='EPSG:4326', source=cx.providers.Esri.WorldStreetMap)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-NrXv3p859"
      },
      "source": [
        "#### Coupez et convertissez le PIXC en un fichier .shp pour permettre son ouverture dans QGIS."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer les logs de Fiona\n",
        "logging.getLogger('fiona').setLevel(logging.ERROR)\n",
        "\n",
        "# Coordonnées de la boîte englobante\n",
        "lat_min = 46.58\n",
        "lat_max = 46.62\n",
        "lon_min = -72.73\n",
        "lon_max = -72.60\n",
        "\n",
        "# Extraite la latitude et la longitude\n",
        "lat = np.asarray(ds_PIXC.latitude[:])\n",
        "lon = np.asarray(ds_PIXC.longitude[:])\n",
        "classif  = np.asarray(ds_PIXC.classification[:])\n",
        "\n",
        "# Définir le masque selon les coordonnées et les variables\n",
        "mask = (lat > lat_min) & (lat < lat_max) & (lon > lon_min) & (lon < lon_max) & (classif>2) & (classif<5)\n",
        "\n",
        "# Créer un dictionnaire avec les variables voulus\n",
        "data = {\n",
        "    'height': np.asarray(ds_PIXC.height[:])[mask],\n",
        "    'classif': np.asarray(ds_PIXC.classification[:])[mask],\n",
        "    'latitude': lat[mask],\n",
        "    'longitude': lon[mask]\n",
        "}\n",
        "\n",
        "# Convertir le dictionnaire de données en DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Créer des géométries et des GeoDataFrame\n",
        "points = [Point(x, y) for x, y in zip(df.longitude, df.latitude)]\n",
        "gdf_out = gpd.GeoDataFrame(df, geometry=points, crs=\"EPSG:4326\")\n",
        "\n",
        "# Sauvegarder en shapefile\n",
        "out_shp = './data_downloads/PIXC_clipped.shp'\n",
        "gdf_out.to_file(out_shp)"
      ],
      "metadata": {
        "id": "5JVeL550CcN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxH9oIeDl-3P"
      },
      "outputs": [],
      "source": [
        "# Affichez vos données PIXC coupées. Vous pouvez aussi les télécharger pour les visualiser dans QGIS.\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "cax= plt.scatter(x=gdf_out.longitude, y=gdf_out.latitude, c=gdf_out.height, s=1) #ajustement de la taille des pixels avec s= taille désirée\n",
        "cbar = fig.colorbar(cax, ax=ax, shrink=0.5)\n",
        "cbar.set_label('Height (m)')\n",
        "\n",
        "# ajustement de la barre de couleur\n",
        "cbar.ax.set_aspect('auto')\n",
        "\n",
        "cx.add_basemap(ax, crs='EPSG:4326', source=cx.providers.Esri.WorldStreetMap)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulakBaLH3rHm"
      },
      "source": [
        "#### **4. Water Mask Pixel Cloud Vector Attribute NetCDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkTelbKM3rHm"
      },
      "source": [
        "#### Recherche des données d'intérêt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqAU4RuT3rHn"
      },
      "outputs": [],
      "source": [
        "pixcvec_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_PIXCVEC_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # peut également être filtré en fonction de la plage horaire\n",
        "                                        granule_name = '*_576_073L*') # numéro de pass, de tuile et côté de fauchée (R ou L)\n",
        "                                        #bounding_box = (-72.73,46.58,-72.60,46.62)) # filtre par boîte englobante, pour trouver votre boîte englobante : http://bboxfinder.com/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26uTWPEj7CMv"
      },
      "outputs": [],
      "source": [
        "#Imprimer les caractéristiques des granules\n",
        "print(pixcvec_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkwvQI1i3rHn"
      },
      "source": [
        "Téléchargez le fichier de données sélectionné ! earthaccess.download a une liste comme format d'entrée, nous devons donc mettre des crochets autour du fichier que nous voulons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SipHO_Lh3rHn"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([pixcvec_results[0]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj1MBbdx3rHn"
      },
      "source": [
        "#### Ouverture des données avec xarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkufSBay3rHn"
      },
      "source": [
        "Nous allons chercher automatiquement le nom du fichier que nous venons de télécharger, puis nous allons l'afficher avec `xarray`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h40-TZA13rHn"
      },
      "outputs": [],
      "source": [
        "ds_PIXCVEC = xr.open_mfdataset(\"data_downloads/SWOT_L2_HR_PIXCVec_*.nc\", decode_cf=False,  engine='h5netcdf')\n",
        "ds_PIXCVEC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btcux1t93rHn"
      },
      "source": [
        "#### Affichage rapide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZWenCYj3rHn"
      },
      "outputs": [],
      "source": [
        "pixcvec_htvals = ds_PIXCVEC.height_vectorproc.compute()\n",
        "pixcvec_latvals = ds_PIXCVEC.latitude_vectorproc.compute()\n",
        "pixcvec_lonvals = ds_PIXCVEC.longitude_vectorproc.compute()\n",
        "\n",
        "#Avant d'afficher le graphique, les valeurs de remplissage sont fixées à NaN (Not a Number) afin d'améliorer l'affichage du graphique dans l'espace.\n",
        "pixcvec_htvals[pixcvec_htvals > 15000] = np.nan\n",
        "pixcvec_latvals[pixcvec_latvals < 1] = np.nan\n",
        "pixcvec_lonvals[pixcvec_lonvals > -1] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtKXWIxM3rHn"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "plt.scatter(x=pixcvec_lonvals, y=pixcvec_latvals, c=pixcvec_htvals, s=1) #ajustement de la taille des pixels avec s= taille désirée\n",
        "plt.colorbar().set_label('Height (m)')\n",
        "\n",
        "cx.add_basemap(ax, crs='EPSG:4326', source=cx.providers.Esri.WorldStreetMap)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfY8ht273rHn"
      },
      "source": [
        "#### **5. Raster NetCDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51PSnIrO3rHo"
      },
      "source": [
        "#### Recherche des données d'intérêt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6EWLsgY3rHo"
      },
      "outputs": [],
      "source": [
        "raster_results = earthaccess.search_data(short_name = 'SWOT_L2_HR_Raster_2.0',\n",
        "                                        #temporal = ('2024-02-01 00:00:00', '2024-02-29 23:59:59'), # peut également être filtré en fonction de la plage temporelle\n",
        "                                        #bounding_box = (-72.73,46.58,-72.60,46.62), # filtre par boîte englobante, pour trouver votre boîte englobante : http://bboxfinder.com/\n",
        "                                        granule_name = '*100m*_576_037F*') # ici nous filtrons les données avec la mention '100m' (Cette collection à deux options de résolution: 100m & 250m)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B43pLnhHC5z"
      },
      "outputs": [],
      "source": [
        "#Imprimer les caractéristiques des granules associées à la passe, à la scène et à la résolution sélectionnées.\n",
        "print(raster_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCOLktF73rHo"
      },
      "source": [
        "Téléchargons un fichier de données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgL6WGbE3rHo"
      },
      "outputs": [],
      "source": [
        "earthaccess.download([raster_results[1]], \"./data_downloads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuvxbYkj3rHo"
      },
      "source": [
        "#### Ouverture des données avec xarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mm9T2Zv3rHo"
      },
      "source": [
        "Nous allons chercher automatiquement le nom du fichier que nous venons de télécharger, puis nous allons l'afficher avec `xarray`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsOMnsqW3rHo"
      },
      "outputs": [],
      "source": [
        "ds_raster = xr.open_mfdataset(f'data_downloads/SWOT_L2_HR_Raster*', engine='h5netcdf')\n",
        "ds_raster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIoYfcIt3rHo"
      },
      "source": [
        "#### Affichage intéractif des données avec `hvplot`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USz9iQ493rHo"
      },
      "outputs": [],
      "source": [
        "hv.extension('bokeh', 'matplotlib')\n",
        "plot = ds_raster['wse'].hvplot.image(y='y', x='x')\n",
        "hv.output(plot)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlNpHaxlPFXX"
      },
      "source": [
        "#### Masquer une variable selon son indicateur de qualité\n",
        "Exemple pour une donnée L2_HR_Raster, indicateur \"wse_qual\":\\\n",
        "0 = bonne\\\n",
        "1 = suspecte -  peut comporter des erreurs importantes\\\n",
        "2 = dégradée - susceptible de contenir des erreurs importantes\\\n",
        "3 = mauvaise - peut ne pas faire de sens et doit être ignorée"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsVbk_B-RpZ3"
      },
      "outputs": [],
      "source": [
        "variable_to_mask = ds_raster['wse']\n",
        "mask_variable = ds_raster['wse_qual']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCX8XB1RV5rd"
      },
      "outputs": [],
      "source": [
        "# Définir la condition pour masquer les données selon l'indicateur de qualité\n",
        "mask_condition = mask_variable < 3\n",
        "masked_variable = variable_to_mask.where(mask_condition)\n",
        "masked_variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFJ0Gf1TWpWR"
      },
      "outputs": [],
      "source": [
        "# Mise à jour de la variable masquée dans l'ensemble de données\n",
        "hv.extension('bokeh', 'matplotlib')\n",
        "plot2 = masked_variable.hvplot.image(y='y', x='x')\n",
        "hv.output(plot2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDn9o1htnTaB"
      },
      "source": [
        "#### Découpez vos données Raster NetCDF masquées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o4jcHoCRGDR"
      },
      "outputs": [],
      "source": [
        "# Créez un fichier pour vos données découpées\n",
        "os.makedirs('./content/clip_data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfdsxxFBCsxM"
      },
      "outputs": [],
      "source": [
        "#Définir la région d'intérêt\n",
        "from shapely.geometry import box\n",
        "\n",
        "ROI = box(-72.73,46.58,-72.60,46.62)\n",
        "bbox_gdf = gpd.GeoDataFrame({'geometry': [ROI]}, crs='EPSG:4326')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFX2Yy1-nZKY"
      },
      "outputs": [],
      "source": [
        "# Définir les dimensions spatiales du jeu de données et le système de référence des coordonnées (crs).\n",
        "masked_variable.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=True)\n",
        "masked_variable.rio.write_crs(\"epsg:32618\", inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qkm1TbdoKzR"
      },
      "outputs": [],
      "source": [
        "# Découper le raster\n",
        "# Si nécessaire, reprojetez d'abord la région d'intérêt dans le même EPSG que celui du netCDF\n",
        "bbox_gdf = bbox_gdf.to_crs(\"epsg:32618\")\n",
        "clipped = masked_variable.rio.clip(bbox_gdf.geometry.apply(mapping), drop=True)\n",
        "\n",
        "#Vérification de la zone d'intérêt recadrée\n",
        "hv.extension('bokeh', 'matplotlib')\n",
        "plot2 = clipped.hvplot.image(y='y', x='x')\n",
        "hv.output(plot2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Otx9IbRsiD7"
      },
      "outputs": [],
      "source": [
        "# Supprimez l'attribut 'grid_mapping'\n",
        "if 'grid_mapping' in clipped.attrs:\n",
        "    del clipped.attrs['grid_mapping']\n",
        "\n",
        "# Sauvegardez le raster découpé en un fichier NetCDF\n",
        "clipped_path = './content/clip_data/clipped_raster.nc'  # Définir le chemin et le nom du fichier à sauvegarder\n",
        "clipped.to_netcdf(clipped_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu09EYYRxxd4"
      },
      "source": [
        "#### **6. Transformation des systèmes de références**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY6Xlk9_yl1o"
      },
      "source": [
        "Le système de référence de SWOT n'est pas le même que celui du Canada. Il est donc nécessaire de convertir les données.\n",
        "**Attention : Chaque agence géodésique provinciale canadienne peut adopter un système de référence vertical et une époque différents !**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ruy3WM3zaOa"
      },
      "source": [
        "Découper les données avec le masque du Québec (nécessaire car chaque province peut adopter une époque et un système de référence vertical différents).\n",
        "\n",
        "Attention : changez le nom du fichier RiverSp pour qu'il corresponde à celui que vous voulez.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdr2jiL1xcie"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('ref_change', exist_ok=True)\n",
        "\n",
        "# Téléchargez le fichier GeoJSON avec les provinces\n",
        "url = 'https://data.opendatasoft.com/api/explore/v2.1/catalog/datasets/georef-canada-province@public/exports/geojson?lang=fr&timezone=America%2FNew_York'\n",
        "\n",
        "gdf_prov = gpd.read_file(url)\n",
        "\n",
        "# Filtrez le GeoDataFrame pour sélectionner seulement celui de la province de Québec\n",
        "quebec_gdf = gdf_prov[gdf_prov['prov_name_fr'] == 'Québec']\n",
        "\n",
        "print(quebec_gdf.geometry)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9t_evD30uJQ"
      },
      "outputs": [],
      "source": [
        "#  Téléchargez le shapefile à découper et vérifiez que le masque et le fichier RiverSp on le même système de coordonnées\n",
        "from shapely.ops import unary_union\n",
        "\n",
        "logging.getLogger('fiona').setLevel(logging.ERROR)\n",
        "\n",
        "shapefile_path = '/content/data_downloads/SWOT_L2_HR_RiverSP_Node_005_576_NA_20231102T054530_20231102T054541_PGC0_01.shp'\n",
        "\n",
        "points_gdf = gpd.read_file(shapefile_path)\n",
        "\n",
        "quebec_gdf = quebec_gdf.to_crs(points_gdf.crs)\n",
        "\n",
        "points_gdf_masked = points_gdf.clip(mask= quebec_gdf.geometry)\n",
        "\n",
        "# Sauvergardez les noeuds selectionnés en format shapefile\n",
        "points_gdf_masked.to_file('/content/ref_change/qc_nodes.shp')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkDhwpMKz4fj"
      },
      "source": [
        "Affichage des données découpées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjh5n79Yz8jp"
      },
      "outputs": [],
      "source": [
        "# Affichage rapide\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "points_gdf_masked.plot(ax=ax, color='black')\n",
        "quebec_gdf.boundary.plot(ax=ax, edgecolor='red')\n",
        "cx.add_basemap(ax, crs=points_gdf_masked.crs, source=cx.providers.Esri.WorldStreetMap)\n",
        "plt.show()\n",
        "\n",
        "#Sauvegardez votre image\n",
        "plt.savefig('/content/clippeddata.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-VqZF6sy3Fq"
      },
      "source": [
        "Ensuite, les données doivent être extraites et sauvegardées dans le bon format pour être utilisées avec l'outil TRX de RNCan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX9hweny8V1Z"
      },
      "outputs": [],
      "source": [
        "# Entrez le chemin de vos données\n",
        "data_all = []\n",
        "\n",
        "with shapefile.Reader('/content/ref_change/qc_nodes.shp') as shp:\n",
        "\n",
        "    fields = [field[0] for field in shp.fields if field[0] != 'DeletionFlag']\n",
        "\n",
        "    for record in shp.records():\n",
        "          attributs = dict(zip(fields, record))\n",
        "          #Selectionnez le node_id, la latitude, la longitude et les wse qui ne sont pas -999999999999\n",
        "          if all(value != -999999999999 for value in attributs.values()):\n",
        "            data_all.append([attributs['node_id'],attributs['lat'],attributs['lon'],attributs['wse']])\n",
        "\n",
        "header = ['Station','latitude','longitude','height']\n",
        "\n",
        "# Création et écriture d'une fichier CSV\n",
        "with open('/content/ref_change/wse_qc.csv', 'w', newline='', encoding='utf-8') as fichier:\n",
        "    writer = csv.writer(fichier)\n",
        "    writer.writerow(header)\n",
        "\n",
        "    # Écriture des données\n",
        "    for data in data_all:\n",
        "        writer.writerow(data)\n",
        "\n",
        "#Sauvergarde dans les fichiers\n",
        "from google.colab import files\n",
        "files.download(\"/content/ref_change/wse_qc.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7rjhEwHzX-3"
      },
      "source": [
        "Enfin, pour utiliser TRX en ligne, rendez-vous sur le site web : https://webapp.csrs-scrs.nrcan-rncan.gc.ca/geod/tools-outils/trx.php?locale=fr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xMIm4Rgy9xx"
      },
      "source": [
        "Remplissez cette information dans la fenêtre **Traitement par lots** :\n",
        "\n",
        "**Origine** :  Cadre de référence : ITRF2014 epoque 01/01/2010\n",
        "\n",
        "**Destination** : Cadre de référence : NAD83(CSRS) Coordinates : Geographic\n",
        "\n",
        " **Transformation d'époque** : 01/01/1997 (époque adoptée pour le Québec, adaptez l'époque en conséquence)\n",
        "\n",
        "Sélectionnez le fichier téléchargé précédemment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Munla4LN81su"
      },
      "source": [
        "#### **7. Téléchargez toutes vos données SWOT sur votre machine locale d'un coup**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF1okpJjGAny"
      },
      "source": [
        "Vous pouvez télécharger les données une par une en cliquant sur les trois petits points à droite du nom du fichier et ensuite sur « Télécharger ». Pour télécharger toutes les données d'un coup, exécutez les cellules suivantes. Les temps d'exécution et de téléchargement peuvent être longs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7fiZ1tk-uSZ"
      },
      "outputs": [],
      "source": [
        "# Créez un fichier zip avec toutes les données et choisissez le fichier à compresser\n",
        "!zip -r /content/data.zip /content/data_downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04WSzmVY-5gz"
      },
      "outputs": [],
      "source": [
        "# Téléchargez le fichier compressé\n",
        "from google.colab import files\n",
        "files.download(\"/content/data.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNw1ksdCoNu2"
      },
      "source": [
        "#### **8. Travailler avec HYDROCON - Rivières**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lP3hh9GY6WI"
      },
      "source": [
        "Extraire des séries temporelles avec Hydrocon pour les Reach et Nodes d'intérêt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import And\n",
        "import folium\n",
        "import requests\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ucPckRl57aR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx2hHQkCoMvF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Vous pouvez choisir un identifiant de Node ou de Reach à partir du produit RiverSP obtenu dans la section 1.\n",
        "#Exemple pour la rivière Saint-Maurice\n",
        "\n",
        "#feature='Node'\n",
        "#feature_id=\"71250300150401\"\n",
        "feature=\"Reach\"\n",
        "feature_id=\"72520100041\"\n",
        "start_time=\"2023-08-01T00:00:00Z\"\n",
        "end_time=\"2024-08-01T00:00:00Z\"\n",
        "#fields=reach_id,time_str,wse,width\n",
        "\n",
        "parameters = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?feature=\"+feature+\"&feature_id=\"+ feature_id +\"&start_time=\" + start_time + \"&end_time=\" + end_time + \"&output=geojson&fields=reach_id,time_str,wse,width,cycle_id\"\n",
        "\n",
        "\n",
        "hydrocron_response = requests.get(\n",
        "    parameters\n",
        ").json()\n",
        "\n",
        "hydrocron_response\n",
        "\n",
        "# Extraction du geojson pour l'afficher sur la carte\n",
        "\n",
        "geojson_data = hydrocron_response['results']['geojson']\n",
        "\n",
        "geojson_data\n",
        "\n",
        "# Configurez la carte à l'aide de Folium (https://python-visualization.github.io/folium/latest/)\n",
        "\n",
        "map = folium.Map (zoom_start=13, tiles=\"cartodbpositron\", width=700, height=700)\n",
        "\n",
        "# Ajoutez le geojson provenant d'Hydrocron à la carte\n",
        "folium.GeoJson(geojson_data, name='SWOT River Reach').add_to(map)\n",
        "folium.LayerControl().add_to(map)\n",
        "\n",
        "# Centrez sur la rivière\n",
        "map.fit_bounds(map.get_bounds(), padding=(5, 5))\n",
        "\n",
        "map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HPkZ9hgZJtw"
      },
      "source": [
        "Afficher une série temporelle de niveau d'eau pour un noeud d'intérêt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJdeAj8RZOzQ"
      },
      "outputs": [],
      "source": [
        "#Exemple pour la rivière Saint-Maurice\n",
        "\n",
        "feature='Node'\n",
        "feature_id=\"72520100230111\" # Rapides-Manigance\n",
        "start_time=\"2023-08-01T00:00:00Z\"\n",
        "end_time=\"2024-08-01T00:00:00Z\"\n",
        "#fields=reach_id,time_str,wse,width\n",
        "\n",
        "#parameters = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?feature=\"+feature+\"&feature_id=\"+ feature_id +\"&start_time=2024-01-01T00:00:00Z&end_time=2024-06-14T00:00:00Z&output=csv&fields=reach_id,node_id,time_str,node_q,wse,width,cycle_id\"\n",
        "\n",
        "parameters = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?feature=\"+feature+\"&feature_id=\"+ feature_id +\"&start_time=\"+start_time+\"&end_time=\"+end_time+\"&output=csv&fields=reach_id,node_id,time_str,node_q,wse,width,cycle_id\"\n",
        "\n",
        "\n",
        "\n",
        "hydrocron_response = requests.get(\n",
        "    parameters\n",
        ").json()\n",
        "\n",
        "hydrocron_response\n",
        "csv_str = hydrocron_response['results']['csv']\n",
        "df = pd.read_csv(StringIO(csv_str))\n",
        "ind = df.node_q<3\n",
        "\n",
        "df = df[df['time_str'] != 'no_data']\n",
        "df.time_str = pd.to_datetime(df.time_str, format='%Y-%m-%dT%H:%M:%SZ')\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "plt.plot(df.time_str[ind], df.wse[ind], marker='o', linestyle='None')\n",
        "\n",
        "plt.ylabel('Water surface elevation (m)')\n",
        "plt.xlabel('SWOT observation date')\n",
        "plt.title('Water Surface Elevation from Hydrocron for Node: ' + str(df.node_id[0]))\n",
        "\n",
        "#Sauvegardez votre image\n",
        "plt.savefig('/content/WSE_Hydrocon.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoVqg9FPLGVp"
      },
      "source": [
        "#### **Extraire un profil entre deux noeuds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUcX173DLLc2"
      },
      "outputs": [],
      "source": [
        "# Créez un fichier pour vos données\n",
        "os.makedirs('./content/profil_node', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zf75nD8Rv3U"
      },
      "source": [
        "Authentification et téléchargement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R1k0Va5MYjZ"
      },
      "outputs": [],
      "source": [
        "# Fonction pour rechercher et télécharger les données\n",
        "def download_data(pass_numbers, continent_code, path, temporal_range):\n",
        "    links_list = []\n",
        "    for pass_num in pass_numbers:\n",
        "        river_results = earthaccess.search_data(\n",
        "            short_name='SWOT_L2_HR_RIVERSP_2.0',\n",
        "            temporal=temporal_range,\n",
        "            granule_name=f\"*Node*_{pass_num}_{continent_code}*\"\n",
        "        )\n",
        "        links_list.extend([earthaccess.results.DataGranule.data_links(result, access='external')[0]\n",
        "                           for result in river_results])\n",
        "\n",
        "    # Téléchargement des fichiers\n",
        "    earthaccess.download(links_list, path)\n",
        "    return links_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q92l9TUR1Is"
      },
      "source": [
        "Extraction des fichiers ZIP et chargement des shapefiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VKLRwl8Mm6C"
      },
      "outputs": [],
      "source": [
        "# Fonction pour extraire les fichiers ZIP\n",
        "def extract_files(links_list, path):\n",
        "    filenames = [link.split(\"/\")[-1] for link in links_list]\n",
        "    for filename in filenames:\n",
        "        with zipfile.ZipFile(f\"{path}/{filename}\", 'r') as zip_ref:\n",
        "            zip_ref.extractall(path)\n",
        "    return filenames\n",
        "\n",
        "# Fonction pour charger les shapefiles\n",
        "def load_shapefiles(filenames, path):\n",
        "    filename_shps = [filename.replace('zip', 'shp') for filename in filenames]\n",
        "    return gpd.GeoDataFrame(pd.concat([gpd.read_file(f\"{path}/{shp}\") for shp in filename_shps], ignore_index=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lexmm0eLR7l-"
      },
      "source": [
        "Filtrage des noeuds et calcul des distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e-y_bc_MsGI"
      },
      "outputs": [],
      "source": [
        "# Fonction pour filtrer les données par noeuds et temps\n",
        "def filter_data_by_nodes(SWOT_HR_df, up_node, dn_node, date=None):\n",
        "    filtered = SWOT_HR_df[(SWOT_HR_df['node_id'] >= dn_node) &\n",
        "                          (SWOT_HR_df['node_id'] < up_node) &\n",
        "                          (SWOT_HR_df['reach_id'] != '72520100361')] #  pour noeud problématique\n",
        "    if date:\n",
        "        filtered = filtered[filtered['time_str'].str.contains(date)]\n",
        "    return filtered.sort_values(['node_id'])\n",
        "\n",
        "def calculate_distances(SWOT_HR_profil):\n",
        "    \"\"\"\n",
        "    Calcule la distance cumulée à partir de la colonne 'p_length' de SWOT_HR_profil_1.\n",
        "\n",
        "    \"\"\"\n",
        "    delta = SWOT_HR_profil.p_length\n",
        "    # Initialiser un tableau de zéros pour stocker les distances cumulées\n",
        "    dist_1=np.zeros((len(SWOT_HR_profil),1))\n",
        "\n",
        "    # Calcul des distances cumulées\n",
        "    for n in range(len(SWOT_HR_profil)-1):\n",
        "      dist_1[n+1]=dist_1[n]+delta.iloc[n]\n",
        "\n",
        "    return dist_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XheFxi3MSAKF"
      },
      "source": [
        "Tracé du profil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v59FAcrHM4us"
      },
      "outputs": [],
      "source": [
        "# Fonction pour tracer le profil de hauteur d'eau\n",
        "def plot_profile(profile, dist, label):\n",
        "    fig, ax = plt.subplots(figsize=(15, 5))\n",
        "    ax.plot(dist, profile['wse'], marker='o', linestyle='None', label=label)\n",
        "    ax.set_xlabel('Distance cumulative (m)')\n",
        "    ax.set_ylabel('Hauteur d\\'eau (wse)')\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "    #Sauvegardez votre image\n",
        "    plt.savefig('/content/profil.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqzSg69hSFGZ"
      },
      "source": [
        "Variables à modifier selon les besoins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hj2QreNNdGZ"
      },
      "outputs": [],
      "source": [
        "# Chemin du répertoire où les données seront enregistrées et variables à identifier\n",
        "path = '/content/profil_node'\n",
        "pass_number    = [\"063\"]  # Liste de numéros de passage (063, 270, 576, 369)\n",
        "continent_code = \"NA\"  # Code du continent (NA pour Amérique du Nord)\n",
        "temporal_range = '2024-04-04 00:00:00', '2024-08-01 23:59:59'\n",
        "dn_node = 72520100020381  # ID du noeud en aval\n",
        "up_node = 72520100020561  # ID du noeud en amont"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjNLJ5ePSNEN"
      },
      "source": [
        "Exécution complète"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA_Ka61WSzL8"
      },
      "outputs": [],
      "source": [
        "# Téléchargement des données\n",
        "links_list = download_data(pass_number, continent_code, path, temporal_range)\n",
        "filenames = extract_files(links_list, path)\n",
        "\n",
        "# Chargement des shapefiles\n",
        "SWOT_HR_df = load_shapefiles(filenames, path)\n",
        "SWOT_HR_df['node_id'] = SWOT_HR_df['node_id'].astype(float)\n",
        "\n",
        "\n",
        "# Filtrage du profil pour la date du 19 avril 2024\n",
        "profile_1 = filter_data_by_nodes(SWOT_HR_df, up_node, dn_node, date='2024-04-19')\n",
        "\n",
        "# Calcul des distances cumulées\n",
        "dist_1 = calculate_distances(profile_1)\n",
        "\n",
        "# Tracé du profil\n",
        "plot_profile(profile_1, dist_1, '19 avril 2024')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **9. Travailler avec HYDROCON - Lacs**"
      ],
      "metadata": {
        "id": "d6WkW4Da13G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE : En raison de la taille du polygone original (L2_HR_LakeSP), seul le point central du lac est renvoyé. Cela vise à faciliter la conformité avec les spécifications de GeoJSON. Les positions des points centraux ne doivent pas être considérés comme exacts."
      ],
      "metadata": {
        "id": "Oc6GZFiw5w6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature=\"PriorLake\"\n",
        "feature_id=\"7251032842\" #LAC CINCONSINE\n",
        "start_time=\"2024-09-01T00:00:00Z\"\n",
        "end_time=\"2024-10-01T00:00:00Z\"\n",
        "\n",
        "parameters = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?feature=\"+feature+\"&feature_id=\"+ feature_id +\"&start_time=\"+start_time+\"&end_time=\"+end_time+\"&output=geojson&fields=lake_id,time_str,wse,area_total\"\n",
        "\n",
        "\n",
        "hydrocron_response = requests.get(\n",
        "    parameters\n",
        ").json()\n",
        "\n",
        "hydrocron_response\n",
        "\n",
        "# Extraction du geojson pour l'afficher sur la carte\n",
        "\n",
        "geojson_data = hydrocron_response['results']['geojson']\n",
        "\n",
        "geojson_data\n",
        "\n",
        "valid_features = [\n",
        "    feature for feature in geojson_data['features']\n",
        "    if float(feature['properties']['wse']) > 0 and  # Filtre sur le wse\n",
        "       -90 <= feature['geometry']['coordinates'][1] <= 90 and  # Latitude valide\n",
        "       -180 <= feature['geometry']['coordinates'][0] <= 180  # Longitude valide\n",
        "]\n",
        "\n",
        "# Créer un nouveau GeoJSON avec les features valides\n",
        "filtered_geojson = {\n",
        "    'type': 'FeatureCollection',\n",
        "    'features': valid_features\n",
        "}\n",
        "\n",
        "# Afficher les features valides pour confirmer\n",
        "filtered_geojson\n",
        "\n",
        "\n",
        "# Configurez la carte à l'aide de Folium (https://python-visualization.github.io/folium/latest/)\n",
        "\n",
        "map = folium.Map (tiles=\"cartodbpositron\", width=700, height=700)\n",
        "\n",
        "# Ajoutez le geojson provenant d'Hydrocron à la carte\n",
        "folium.GeoJson(filtered_geojson, name='SWOT Prior Lake').add_to(map)\n",
        "folium.LayerControl().add_to(map)\n",
        "\n",
        "\n",
        "# Centrez sur la rivière\n",
        "map.fit_bounds(map.get_bounds(), padding=(5, 5))\n",
        "\n",
        "map\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HZ7xYKHR2Oe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher une série temporelle de niveau d'eau pour un noeud d'intérêt"
      ],
      "metadata": {
        "id": "zOodWeIU4xYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from io import StringIO\n",
        "\n",
        "# Definir les paramètres\n",
        "feature = \"PriorLake\"\n",
        "feature_id = \"7251032842\"\n",
        "start_time = \"2023-09-01T00:00:00Z\"\n",
        "end_time = \"2024-10-01T00:00:00Z\"\n",
        "parameters = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries?feature=\" + feature + \"&feature_id=\" + feature_id + \"&start_time=\" + start_time + \"&end_time=\" + end_time + \"&output=csv&fields=lake_id,time_str,wse,area_total,quality_f,dark_frac\"\n",
        "\n",
        "\n",
        "hydrocron_response = requests.get(parameters).json()\n",
        "\n",
        "# Extraire le CSV et créer le DataFrame\n",
        "csv_str = hydrocron_response['results']['csv']\n",
        "df = pd.read_csv(StringIO(csv_str))\n",
        "\n",
        "# Filtrer les entrées où 'time_str' est différent de 'no_data'\n",
        "df = df[df['time_str'] != 'no_data']\n",
        "# Filtrer les entrées où 'quality_f' est égal à 0 (bon)\n",
        "df = df[df['quality_f'] == 0]\n",
        "# Filtrer les entrées où 'dark_frac' est inférieure à 50%\n",
        "df = df[df['dark_frac'] < 0.5]\n",
        "# Convertir 'time_str' au format datetime\n",
        "df['time_str'] = pd.to_datetime(df['time_str'], format='%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "# Créer la figure\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "plt.plot(df['time_str'], df['wse'], marker='o', linestyle='None')\n",
        "\n",
        "\n",
        "plt.ylabel('Water surface elevation (m)')\n",
        "plt.xlabel('SWOT observation date')\n",
        "plt.title('Water Surface Elevation from Hydrocron for Lake: ' + str(df['lake_id'].iloc[0]))\n",
        "\n",
        "# Sauver l'image\n",
        "plt.savefig('/content/WSE_Hydrocron_Lake.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K_aNk0JcKv0-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5a4443810289f87e0f862ef34d31d94a0884467de587e41820bef73e0713c5c1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}